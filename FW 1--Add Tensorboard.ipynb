{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Adding Tensorboard to the mix\n\nIn this notebook, we will add support for Tensorboard to allow us to track our results across multiple tests.  We we said in [Model Definition 2](https://github.com/adamx97/Data-Science-Advanced-Capstone/blob/master/Model%20Definition2--Advanced%20Data%20Science%20Capstone.ipynb)\n\n> \"We need to track the metrics of various models and parameter combinations, with the ability to backtrack to a former configuration if we find we are going down the wrong path.  We will use Tensorboard for this.\"\n\nWe want to be able to easily add models and track their evolution and performance in Tensorboard.  We will start with the same models we used in the Model Definistion 2 notebook, but add Tensorboard support.\n\n\n## Initial Model Exploration \nOur research into initial models for Text Classification used two models shown below:\n\n### Phase 1: First test model: Embedding, 1D Convolutional model with GlobalMaxPooling\n\nThis model was based on a sample Text Classification task that we found.  We vectorized our data with a simple integer encoding.  It uses embedding to turn our input vectors into dense vectors of fixed size.  Word embeddings encode each word into a dense vector that learns about its relative meaning within the training text.  By sliding a convolutional window over the vectors, there is the increased possibility that training will learn the meaning of various word combinations more easily than single or double word (our ngrams) occurances.  Our activation layer uses sigmoid to refine our results to a binary classification.\n\nx = layers.Embedding(max_features + 1, embedding_dim)(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3)(x)\nx = layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3)(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\npredictions = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n\n\nThe results of our iniitial tests with this model are shown below.\n\n### Phase 2: Second test model: \nThis model was based on a second sample Text Classification task that we found.  We vectorized our data using tf-idf (term-frequency times inverse document-frequency), a common weighting scheme used for document classification (see Feature Engineering notebook for more).  It uses fully 3 connected dense layers alternating with Dropout layers. We experimented with tanh and relu activations in this notebook.  We will further test varying the dropout rate, possibly testing it at each layer. As above, our activation layer uses sigmoid to refine our results to a binary classification.\n\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='tanh')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\npredictions = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n\nThe results of our iniitial tests with this model are shown below.\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade numpy\n!pip install --upgrade pandas\n\n# we want tensorflow 2.3\n!pip install --upgrade tensorflow  ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf\nprint(\"Tensorflow version: \", tf.__version__)\nif not tf.__version__ == '2.3.0':\n    raise ValueError('please upgrade to TensorFlow 2.3, or restart your Kernel (Kernel->Restart & Clear Output)')\nfrom tensorflow import keras\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Tensorflow version:  2.3.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom time import time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport string\nimport re\nfrom datetime import datetime\nfrom packaging import version\n\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Get our data\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_news = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-32e8ee67-397c-4ff1-b69b-543172331f43',\n    'IBM_API_KEY_ID': 'Rx4FR4JSAueCnnIsoevsgYgOsuh8LCXtbkFpFpC0EmVU',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'advanceddatasciencecapstone-donotdelete-pr-tqabpnbxebk8rm',\n    'FILE': 'dfTrueFalseNews.pkl'\n}\n\ndef download_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.download_file(Bucket=credentials['BUCKET'],Key=key,Filename=local_file_name)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print('File Downloaded')\n\ndef upload_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.upload_file(Filename=local_file_name, Bucket=credentials['BUCKET'],Key=key)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print(' File Uploaded')\n        \ndfNews = download_file_cos(credentials_news, \"dfTrueFalseNews.pkl\", \"dfTrueFalseNews.pkl\")",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNews = pd.read_pickle('dfTrueFalseNews.pkl')\n#dfNews['truthvalue'] = pd.Categorical(dfNews['truthvalue'])\n\nprint (dfNews.shape, dfNews.columns, '\\n',  dfNews.dtypes)",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(1126, 3) Index(['text', 'source', 'truthvalue'], dtype='object') \n text          object\nsource        object\ntruthvalue    object\ndtype: object\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNews.head()",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/plain": "                                                           text  \\\ntech003legit  A Google computer victorious over the world's ...   \npolit11legit  White House keeps up sanctuary cities pressure...   \nbiz40legit    Why Silicon Valley isn't fighting to save the ...   \nedu10legit    Protesters Disrupt DeVos School Visit   Protes...   \ntech038legit  Solar-powered 'skin' could make prosthetics mo...   \n\n                         source truthvalue  \ntech003legit  MihalceaNewsLegit          1  \npolit11legit  MihalceaNewsLegit          1  \nbiz40legit    MihalceaNewsLegit          1  \nedu10legit    MihalceaNewsLegit          1  \ntech038legit  MihalceaNewsLegit          1  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>A Google computer victorious over the world's ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>White House keeps up sanctuary cities pressure...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>Why Silicon Valley isn't fighting to save the ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>Protesters Disrupt DeVos School Visit   Protes...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>Solar-powered 'skin' could make prosthetics mo...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "x = dfNews['text'].values\ny = dfNews['truthvalue'].values\nprint(type(x), type(y))",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n# Once we have our handles, we format the datasets in a Keras-fit compatible\n# format: a tuple of the form (text_data, label).\ndef format_dataset(x, y):\n  return (x, y)\n\ntrain_dataset = list(map(format_dataset, X_train, y_train))\ntest_dataset = list(map(format_dataset, X_test, y_test))\n\n# We also create a dataset with only the textual data in it. This will be used\n# to build our vocabulary later on.\ntextL_dataset = list(map(lambda a:a, x))\n",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print (len(X_train), len(X_test), len(y_train), len(y_test), len(textL_dataset), '\\n',\ntype(X_train), type(X_test), type(y_train), type(y_test), type(textL_dataset))\n",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "900 226 900 226 1126 \n <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'list'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# move our numpy structures into Tensorflow datasets\nDataset = tf.data.Dataset\ntext_dataset = tf.data.Dataset.from_tensor_slices(textL_dataset)\n\nfeatures_train_dataset = Dataset.from_tensor_slices(X_train)\nlabels_train_dataset = Dataset.from_tensor_slices(list(y_train))\ntfds_train = Dataset.zip((features_train_dataset, labels_train_dataset))\n\nfeatures_test_dataset = Dataset.from_tensor_slices(X_test)\nlabels_test_dataset = Dataset.from_tensor_slices(list(y_test))\ntfds_test = Dataset.zip((features_test_dataset, labels_test_dataset))",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Phase 1: Adding Tensorboard to Model 1  basic vectorization and Embedding and Convolutional 1d model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras import layers\n\n# Model constants.\nmax_features = 20000 #vocabulary size\nembedding_dim = 128\n\n# We are using this layer to normalize, split, and map\n# strings to integers, so we set our 'output_mode' to 'int'.\n# Note that we're using the default split function,\n# and the custom standardization defined above.\n# We also set an explicit maximum sequence length, since the CNNs later in our\n# model won't support ragged sequences.\nvectorize_layer1 = TextVectorization(\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=500)\n\n# Now that the vocab layer has been created, call `adapt` on the text-only\n# dataset to create the vocabulary. You don't have to batch, but for very large\n# datasets this means you're not keeping spare copies of the dataset in memory.\nvectorize_layer1.adapt(text_dataset.batch(64))\n\n\n# A text input.\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n\n# The first layer in our model is the vectorization layer. After this layer,\n# we have a tensor of shape (batch_size, max_len) containing vocab indices.\nx = vectorize_layer1(text_input)\n\n# Next, we add a layer to map those vocab indices into a space of dimensionality\n# 'embedding_dim'. Note that we're using max_features+1 here, since there's an\n# OOV token that gets added to the vocabulary in vectorize_layer.\nx = layers.Embedding(max_features + 1, embedding_dim)(x)\nx = layers.Dropout(0.5)(x)\n\n# Conv1D + global max pooling\nx = layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3)(x)\n\nx = layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3)(x)\nx = layers.GlobalMaxPooling1D()(x)\n\n# We add a vanilla hidden layer:\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\npredictions = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n\nmodel1 = tf.keras.Model(text_input, predictions)\n\n# Compile the model with binary crossentropy loss and an adam optimizer.\nmodel1.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "batch_size = 32\nepochs = 1\n\n# Tensorboard callback\nlogdir = \"logs/scalars/model1_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n\n# Fit the model using the train and test datasets.\ntraining_history = model1.fit(\n    tfds_train.batch(batch_size),\n    validation_data=tfds_test.batch(batch_size),\n    epochs=epochs,\n    callbacks=[tensorboard_callback])\n\nprint(\"Average test loss: \", np.average(training_history.history['loss']))",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "\r 1/29 [>.............................] - ETA: 0s - loss: 0.6936 - accuracy: 0.4062",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "WARNING: Logging before flag parsing goes to stderr.\nW0901 20:28:01.829129 140568521832256 deprecation.py:323] From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nuse `tf.profiler.experimental.stop` instead.\n",
                    "name": "stderr"
                },
                {
                    "output_type": "stream",
                    "text": "29/29 [==============================] - 8s 269ms/step - loss: 0.6965 - accuracy: 0.4711 - val_loss: 0.6954 - val_accuracy: 0.4823\nAverage test loss:  0.6964858174324036\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Model 1 using 2 ngrams\nvectorize_layer1 = TextVectorization(\n    ngrams=2,\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=500)\n\n# Now that the vocab layer has been created, call `adapt` on the text-only\n# dataset to create the vocabulary. You don't have to batch, but for very large\n# datasets this means you're not keeping spare copies of the dataset in memory.\nvectorize_layer1.adapt(text_dataset.batch(64))\n\nbatch_size = 64\nepochs = 3\n\n# Fit the model using the train and test datasets.\nmodel1.fit(\n    tfds_train.batch(batch_size),\n    validation_data=tfds_test.batch(batch_size),\n    epochs=epochs)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Epoch 1/3\n15/15 [==============================] - 5s 346ms/step - loss: 0.6896 - accuracy: 0.5567 - val_loss: 0.6966 - val_accuracy: 0.4823\nEpoch 2/3\n15/15 [==============================] - 5s 340ms/step - loss: 0.6918 - accuracy: 0.5244 - val_loss: 0.6939 - val_accuracy: 0.4867\nEpoch 3/3\n15/15 [==============================] - 5s 323ms/step - loss: 0.6883 - accuracy: 0.5433 - val_loss: 0.6967 - val_accuracy: 0.4867\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fd7d889ec88>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade grpcio",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting grpcio\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/48/5aae2b4f415cdab711ec9ec762f433b5d55184ec6e91afa3bc1092d1d0ab/grpcio-1.31.0-cp36-cp36m-manylinux2010_x86_64.whl (3.3MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3MB 6.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from grpcio) (1.12.0)\nInstalling collected packages: grpcio\n  Found existing installation: grpcio 1.16.1\n    Uninstalling grpcio-1.16.1:\n      Successfully uninstalled grpcio-1.16.1\nSuccessfully installed grpcio-1.31.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%tensorboard --logdir logs/scalars\n#S3_VERIFY_SSL=0 S3_ENDPOINT=s3-api.us-geo.objectstorage.softlayer.net AWS_ACCESS_KEY_ID=xxxxx AWS_SECRET_ACCESS_KEY=xxxxx AWS_REGION=us-geo tensorboard --logdir=s3://mnist-fashion-results/training--1u0V9RiR/learner-1/logs/tb > /dev/null",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "Reusing TensorBoard on port 6006 (pid 520), started 0:00:45 ago. (Use '!kill 520' to kill it.)"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "\n      <iframe id=\"tensorboard-frame-1b921514e5076281\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1b921514e5076281\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!cat logs/scalars/model1_20200901-202800/validation/events.out.tfevents.1598992088.notebook-a7d2ab4351974ce48182be28fae05a5d-78b569bc5b-5f7jq.172.1621.v2\n",
            "execution_count": 28,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\ufffd\u007fK\"\t\u0000\u0000\u0000\ufffd\ufffd\ufffd\ufffdA\u001a\rbrain.Event:2b\u0131\ufffd\u001e\u0000\u0000\u0000\u0000\u0000\u0000\u0000\ufffd\ufffd\u00122\t_\ufffdR\ufffd\ufffd\ufffd\ufffdA*\u0013\r\n\u0011\r\n\r\nepoch_loss\u0015\ufffd\u00072?\ufffd&36\"\u0000\u0000\u0000\u0000\u0000\u0000\u0000x\u0011=\ufffd\t\ufffd\ufffdR\ufffd\ufffd\ufffd\ufffdA*\u0017\r\n\u0015\r\n\u000eepoch_accuracy\u0015$\ufffd\ufffd>\ufffd\ufffd\u0001T",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Phase 1: Model 1 Discussion\nThis is the first model we tested and the two runs shown above seemed to reach peak accuracy in the 2 to 3 epoch range, though we will test them further as we move forward.  Using 2 ngrams (vs. single words) in our TextVectorization layer improved accuracy about 1.8%."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Model 2 TextVectorization layer in a bigram TF-IDF densely-connected model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras import layers\n\n\n# Model constants.\nmax_features = 14745 #vocabulary size\n\n\n#We are using this layer to normalize, split, and map\n# strings to integers, so we set our 'output_mode' to 'int'.\n# Note that we're using the default split function,\n# and the custom standardization defined above.\n# We also set an explicit maximum sequence length, since the CNNs later in our\n# model won't support ragged sequences.\nvectorize_layer2 = TextVectorization(\n    max_tokens=max_features,\n    output_mode='tf-idf',\n    ngrams=2,  # Unigrams and bigrams\n    \n    #output_sequence_length=500\n    )\n\n# Now that the vocab layer has been created, call `adapt` on the text-only\n# dataset to create the vocabulary. You don't have to batch, but for very large\n# datasets this means you're not keeping spare copies of the dataset in memory.\nvectorize_layer2.adapt(text_dataset.batch(64))\n\n\n\n# A text input.\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n\n# The first layer in our model is the vectorization layer. After this layer,\n# we have a tensor of shape (batch_size, features) containing TF-IDF features.\nx = vectorize_layer2(text_input)\n\n# Dense hidden layers\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='tanh')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\npredictions = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n\nmodel2 = tf.keras.Model(text_input, predictions)\n\n# Compile the model with binary crossentropy loss and an adam optimizer.\nmodel2.compile(\n    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "batch_size = 64\nepochs = 6\n\n# Fit the model using the train and test datasets.\nhistory = model2.fit(\n    tfds_train.batch(batch_size),\n    validation_data=tfds_test.batch(batch_size),\n    epochs=epochs)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#results = model2.evaluate(tfds_test)\n\n#print(results)\nhistory_dict = history.history\nhistory_dict.keys()\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Results with 4 layers, 3 epochs: \n\nLayers were: \nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\n\nAccuracy is improving, but validation accuracy is not improving,  seems like overfitting.\n`  Epoch 1/3\n  15/15 [==============================] - 39s 3s/step - loss: 1.1349 - accuracy: 0.5039 - val_loss: 0.6969 - val_accuracy: 0.5531\n  Epoch 2/3\n  15/15 [==============================] - 39s 3s/step - loss: 0.7954 - accuracy: 0.6246 - val_loss: 0.7491 - val_accuracy: 0.5310\n  Epoch 3/3\n  15/15 [==============================] - 39s 3s/step - loss: 0.5594 - accuracy: 0.7619 - val_loss: 0.7448 - val_accuracy: 0.5354\n `"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Results with 6 layers, 3 epochs 128 units dense layer.\nLayers were: \n\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\nBoth accuracy and val_accuracy are improving, seems better.\n`\nEpoch 1/3\n15/15 [==============================] - 38s 3s/step - loss: 1.0789 - accuracy: 0.5028 - val_loss: 0.7001 - val_accuracy: 0.5265\nEpoch 2/3\n15/15 [==============================] - 37s 2s/step - loss: 0.9954 - accuracy: 0.5205 - val_loss: 0.6982 - val_accuracy: 0.5221\nEpoch 3/3\n15/15 [==============================] - 37s 2s/step - loss: 0.9662 - accuracy: 0.5692 - val_loss: 0.6842 - val_accuracy: 0.5708\n`"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Results with 6 layers, 3 epochs, 256 more units in the Dense layer. 18000 vocab\n\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\n`\nEpoch 1/3\n15/15 [==============================] - 35s 2s/step - loss: 1.3553 - accuracy: 0.4795 - val_loss: 0.7590 - val_accuracy: 0.4956\nEpoch 2/3\n15/15 [==============================] - 35s 2s/step - loss: 1.0434 - accuracy: 0.5703 - val_loss: 0.6703 - val_accuracy: 0.5575\nEpoch 3/3\n15/15 [==============================] - 35s 2s/step - loss: 0.8842 - accuracy: 0.6301 - val_loss: 0.6928 - val_accuracy: 0.5265\n`"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Results with 6 layers, 3 epochs, 256 units in Dense layer, and all single occurring words removed, leaving 14,745 vocabulary words.\n`\nEpoch 1/3\n15/15 [==============================] - 29s 2s/step - loss: 1.2235 - accuracy: 0.4906 - val_loss: 0.8649 - val_accuracy: 0.4823\nEpoch 2/3\n15/15 [==============================] - 28s 2s/step - loss: 1.0063 - accuracy: 0.5847 - val_loss: 0.6744 - val_accuracy: 0.6062\nEpoch 3/3\n15/15 [==============================] - 28s 2s/step - loss: 0.7222 - accuracy: 0.6611 - val_loss: 0.6706 - val_accuracy: 0.5664\n`"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Replace dense layer with tanh activation layer:\n\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='tanh')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\nResults with 6 layers, 6 epochs, 256 units in Dense layer, middle layer w/ tanh activation and all single occurring words removed, leaving 14,745 vocabulary words.\n`\nEpoch 1/6\n15/15 [==============================] - 29s 2s/step - loss: 0.8455 - accuracy: 0.5083 - val_loss: 0.6888 - val_accuracy: 0.5221\nEpoch 2/6\n15/15 [==============================] - 28s 2s/step - loss: 0.7452 - accuracy: 0.5703 - val_loss: 0.7091 - val_accuracy: 0.5000\nEpoch 3/6\n15/15 [==============================] - 28s 2s/step - loss: 0.7064 - accuracy: 0.6213 - val_loss: 0.6763 - val_accuracy: 0.5796\nEpoch 4/6\n15/15 [==============================] - 28s 2s/step - loss: 0.5463 - accuracy: 0.7276 - val_loss: 0.7541 - val_accuracy: 0.5619\nEpoch 5/6\n15/15 [==============================] - 28s 2s/step - loss: 0.3879 - accuracy: 0.8295 - val_loss: 0.9690 - val_accuracy: 0.5841\nEpoch 6/6\n15/15 [==============================] - 28s 2s/step - loss: 0.2275 - accuracy: 0.9037 - val_loss: 1.1818 - val_accuracy: 0.5929\n`"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "history_dict = history.history\nhistory_dict.keys()\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Results with 6 layers, 3 epochs, 256 more units in the Dense layer. 20000 vocab\n\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n`\nEpoch 1/3\n15/15 [==============================] - 38s 3s/step - loss: 1.2857 - accuracy: 0.5316 - val_loss: 0.7208 - val_accuracy: 0.5398\nEpoch 2/3\n15/15 [==============================] - 38s 3s/step - loss: 1.1039 - accuracy: 0.5703 - val_loss: 0.6790 - val_accuracy: 0.5664\nEpoch 3/3\n15/15 [==============================] - 38s 3s/step - loss: 0.9360 - accuracy: 0.6523 - val_loss: 0.6857 - val_accuracy: 0.5885\n`"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Results with 6 layers, 3 epochs, 256 units in dense layer, 25000 vocab.\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n`\nEpoch 1/3\n15/15 [==============================] - 49s 3s/step - loss: 1.6061 - accuracy: 0.5017 - val_loss: 0.6888 - val_accuracy: 0.5044\nEpoch 2/3\n15/15 [==============================] - 48s 3s/step - loss: 1.0667 - accuracy: 0.5415 - val_loss: 0.7125 - val_accuracy: 0.4912\nEpoch 3/3\n15/15 [==============================] - 48s 3s/step - loss: 1.0003 - accuracy: 0.6456 - val_loss: 0.6951 - val_accuracy: 0.5221\n`",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model2.summary()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Phase 2: Model 2 Discussion\n\nThis is the second model we tested. The primary difference from the first model is the textvectorization (tf-idf weighting vs. unweighted integer) and fully connected dense layers vs convolutional wones.   Here we extended our epochs to 6 and found that indeed, the 2nd or 3rd epoch tended to give the best results.  \n \nWe also varied activation layers (relu vs. tanh) and vocabulary size.  \n\nMaking multiple changes to our test in multiple dimensions made clear the challenges of maintaining clear notes concerning the model parameters and results obtained. the two runs shown above seemed to reach peak accuracy in the 2 to 3 epoch range, though we will test them further as we move forward. \n\nThus we are motivated to put together a clearly structured lab environment for testing models, parameters, and storing results.  Luckily Tensorflow provides much of the framework for this.  This work is continued in the next notebook.\n\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}