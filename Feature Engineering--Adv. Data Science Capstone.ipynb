{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Feature Engineering--True and False News\nThe final step of feature engineering is to tokenize the text of the stories.  The raw data sequence of characters cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n\nI considered doing this using discrete Scikit-Learn modules, but the recently Tensorflow 2.1 adds support for a TextVectorization layer, and 2.3 adds experiment support for the new Keras Preprocessing Layers API. These layers allow you to package preprocessing logic inside the model for easier deployment \u2014 allowing the model to take raw strings, images, or rows from a table as input.  This module also includes a \n\nThe processing of each sample contains the following steps:\n\n1. Standardize each sample.  Lowercase all words and strip punctuation. \n\n2. Split each sample into substrings (usually words).\n\n3. Recombine substrings into tokens (usually ngrams). Options here include determining how many words to include in each token.  Text classification tasks typically  consider tokens of 1 or 2 works, but we may experiment with more than that.\n\n4. Index tokens (associate a unique int value with each token).\n\n5. Transform each sample using this index, either into a vector of ints or a dense float vector.  This layer includes the ability to set the length of the resulting vector, either truncating or padding the vector with zeroes so it will fit the size of our input layer.  It also has several output modes, including tf-idf which is weighting algorithm based on the frequency of words found on the dataset.\n\nFrom the Scikit-learn documentation: \n> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.  In a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\nIn order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf\u2013idf transform.\n\nWe will consider varying output modes as we go forward. \n\nSince the TextVectorization layer will allow us to convert the text of our stories to integer tensors, so there is not much for us to do with feature engineering.  \n\n## Vocabulary Size\nThe default TextVectorization settings will retain all words found as part of our vocabulary.  In experimenting, we found some value in altering the vocabulary size.  \n\nBelow is the code we used to count words and find words that only appeared once, with the hypothesis that any word that appeared in only one article could not inform decisions on any other articles. The code and some further discussion is found below. "
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "!pip install --upgrade numpy\n!pip install --upgrade pandas\n\n# we want tensorflow 2.3\n!pip install --upgrade tensorflow  ",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting numpy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5MB 9.9MB/s eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                    | 5.1MB 9.9MB/s eta 0:00:01\n\u001b[31mERROR: tensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mERROR: autoai-libs 1.10.5 has requirement pandas>=0.24.2, but you'll have pandas 0.24.1 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Found existing installation: numpy 1.15.4\n    Uninstalling numpy-1.15.4:\n      Successfully uninstalled numpy-1.15.4\nSuccessfully installed numpy-1.19.1\nCollecting pandas\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/c6/9ac4ae44c24c787a1738e5fb34dd987ada6533de5905a041aa6d5bea4553/pandas-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.5MB 8.3MB/s eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c           | 6.7MB 8.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2018.9)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: numpy>=1.15.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (1.19.1)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\nInstalling collected packages: pandas\n  Found existing installation: pandas 0.24.1\n    Uninstalling pandas-0.24.1:\n      Successfully uninstalled pandas-0.24.1\nSuccessfully installed pandas-1.1.1\nCollecting tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/ae/0b08f53498417914f2274cc3b5576d2b83179b0cbb209457d0fde0152174/tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 320.4MB 63kB/s  eta 0:00:013                  | 27.7MB 9.5MB/s eta 0:00:31     |\u2588\u2588\u2588\u258f                            | 31.9MB 18.5MB/s eta 0:00:16     |\u2588\u2588\u2588\u258e                            | 32.9MB 18.5MB/s eta 0:00:16     |\u2588\u2588\u2588\u2588                            | 39.5MB 18.5MB/s eta 0:00:16     |\u2588\u2588\u2588\u2588\u258b                           | 46.3MB 18.5MB/s eta 0:00:15     |\u2588\u2588\u2588\u2588\u258a                           | 47.3MB 18.5MB/s eta 0:00:15     |\u2588\u2588\u2588\u2588\u2589                           | 48.3MB 18.5MB/s eta 0:00:15     |\u2588\u2588\u2588\u2588\u2588\u258e                          | 53.4MB 25.9MB/s eta 0:00:11     |\u2588\u2588\u2588\u2588\u2588\u258c                          | 55.0MB 25.9MB/s eta 0:00:11     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                  | 135.6MB 59.9MB/s eta 0:00:04     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                  | 136.3MB 59.9MB/s eta 0:00:04     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                | 157.4MB 27.0MB/s eta 0:00:07     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b               | 166.3MB 27.0MB/s eta 0:00:06     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e             | 183.4MB 17.0MB/s eta 0:00:09MB/s eta 0:00:02     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b        | 236.5MB 27.1MB/s eta 0:00:04     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e       | 242.5MB 27.1MB/s eta 0:00:03     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589       | 248.2MB 4.1MB/s eta 0:00:18     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 310.7MB 7.4MB/s eta 0:00:02\n\u001b[?25hCollecting google-pasta>=0.1.8 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 27.4MB/s eta 0:00:01\n\u001b[?25hCollecting astunparse==1.6.3 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\nRequirement already satisfied, skipping upgrade: wheel>=0.26 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.32.3)\nCollecting tensorboard<3,>=2.3.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 49.3MB/s eta 0:00:01\n\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 12.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (3.11.2)\nRequirement already satisfied, skipping upgrade: six>=1.12.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.12.0)\nRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.7.0)\nCollecting tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 460kB 23.9MB/s eta 0:00:01\n\u001b[?25hCollecting gast==0.3.3 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.16.1)\nCollecting opt-einsum>=2.3.2 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 13.9MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.11.1)\nCollecting scipy==1.4.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1MB 28.4MB/s eta 0:00:01eta 0:00:02     |\u2588\u2588\u2588\u2588\u2588\u258f                          | 4.2MB 22.8MB/s eta 0:00:01     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f       | 19.7MB 28.4MB/s eta 0:00:01     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.0MB 28.4MB/s eta 0:00:01\n\u001b[?25hCollecting h5py<2.11.0,>=2.10.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 20.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\nCollecting numpy<1.19.0,>=1.16.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.1MB 8.6MB/s eta 0:00:011\n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\nRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.21.0)\nCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 788kB 52.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.14.1)\nCollecting setuptools>=41.0.0 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8b/379494d7dbd3854aa7b85b216cb0af54edcb7fce7d086ba3e35522a713cf/setuptools-50.0.0-py3-none-any.whl (783kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 788kB 50.4MB/s eta 0:00:01/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.0.1)\nCollecting google-auth<2,>=1.6.3 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/7f/ef6bcf2cc0f50c7163afb94382aab67a6b278e1e447c2e3981aa281b9747/google_auth-1.21.0-py2.py3-none-any.whl (92kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 32.7MB/s ta 0:00:01\n\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\nRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\nRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.1)\nCollecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\nCollecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 14.2MB/s eta 0:00:01\n\u001b[?25hCollecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 57.6MB/s eta 0:00:01\n\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 51.2MB/s eta 0:00:01\n\u001b[?25hCollecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 14.0MB/s eta 0:00:01\n\u001b[31mERROR: tensorboard 2.3.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.16.1 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: google-pasta, astunparse, cachetools, pyasn1, rsa, pyasn1-modules, setuptools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, numpy, tensorboard, keras-preprocessing, tensorflow-estimator, gast, opt-einsum, scipy, h5py, tensorflow\n  Found existing installation: astunparse 1.6.2\n    Uninstalling astunparse-1.6.2:\n      Successfully uninstalled astunparse-1.6.2\n  Found existing installation: setuptools 40.8.0\n    Uninstalling setuptools-40.8.0:\n      Successfully uninstalled setuptools-40.8.0\n  Found existing installation: numpy 1.19.1\n    Uninstalling numpy-1.19.1:\n      Successfully uninstalled numpy-1.19.1\n  Found existing installation: Keras-Preprocessing 1.0.5\n    Uninstalling Keras-Preprocessing-1.0.5:\n      Successfully uninstalled Keras-Preprocessing-1.0.5\n  Found existing installation: tensorflow-estimator 1.13.0\n    Uninstalling tensorflow-estimator-1.13.0:\n      Successfully uninstalled tensorflow-estimator-1.13.0\n  Found existing installation: gast 0.2.2\n    Uninstalling gast-0.2.2:\n      Successfully uninstalled gast-0.2.2\n  Found existing installation: scipy 1.2.0\n    Uninstalling scipy-1.2.0:\n      Successfully uninstalled scipy-1.2.0\n  Found existing installation: h5py 2.9.0\n    Uninstalling h5py-2.9.0:\n      Successfully uninstalled h5py-2.9.0\n  Found existing installation: tensorflow 1.13.1\n    Uninstalling tensorflow-1.13.1:\n      Successfully uninstalled tensorflow-1.13.1\nSuccessfully installed astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.21.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 h5py-2.10.0 keras-preprocessing-1.1.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 setuptools-50.0.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf\nprint(\"Tensorflow version: \", tf.__version__)\nif not tf.__version__ == '2.3.0':\n    raise ValueError('please upgrade to TensorFlow 2.3, or restart your Kernel (Kernel->Restart & Clear Output)')",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Tensorflow version:  2.3.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom time import time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport string\nimport re\n\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Get our data\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_news = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-32e8ee67-397c-4ff1-b69b-543172331f43',\n    'IBM_API_KEY_ID': 'Rx4FR4JSAueCnnIsoevsgYgOsuh8LCXtbkFpFpC0EmVU',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'advanceddatasciencecapstone-donotdelete-pr-tqabpnbxebk8rm',\n    'FILE': 'dfTrueFalseNews.pkl'\n}\n\ndef download_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.download_file(Bucket=credentials['BUCKET'],Key=key,Filename=local_file_name)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print('File Downloaded')\n\ndef upload_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.upload_file(Filename=local_file_name, Bucket=credentials['BUCKET'],Key=key)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print(' File Uploaded')\n        \ndfNews = download_file_cos(credentials_news, \"dfTrueFalseNews.pkl\", \"dfTrueFalseNews.pkl\")",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNews = pd.read_pickle('dfTrueFalseNews.pkl')\n#dfNews['truthvalue'] = pd.Categorical(dfNews['truthvalue'])\n\nprint (dfNews.shape, dfNews.columns, '\\n',  dfNews.dtypes)",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(1126, 3) Index(['text', 'source', 'truthvalue'], dtype='object') \n text          object\nsource        object\ntruthvalue    object\ndtype: object\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNews.head()",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "                                                           text  \\\ntech003legit  A Google computer victorious over the world's ...   \npolit11legit  White House keeps up sanctuary cities pressure...   \nbiz40legit    Why Silicon Valley isn't fighting to save the ...   \nedu10legit    Protesters Disrupt DeVos School Visit   Protes...   \ntech038legit  Solar-powered 'skin' could make prosthetics mo...   \n\n                         source truthvalue  \ntech003legit  MihalceaNewsLegit          1  \npolit11legit  MihalceaNewsLegit          1  \nbiz40legit    MihalceaNewsLegit          1  \nedu10legit    MihalceaNewsLegit          1  \ntech038legit  MihalceaNewsLegit          1  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>A Google computer victorious over the world's ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>White House keeps up sanctuary cities pressure...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>Why Silicon Valley isn't fighting to save the ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>Protesters Disrupt DeVos School Visit   Protes...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>Solar-powered 'skin' could make prosthetics mo...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "x = dfNews['text'].values\ny = dfNews['truthvalue'].values\nprint(type(x), type(y))",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n# Once we have our handles, we format the datasets in a Keras-fit compatible\n# format: a tuple of the form (text_data, label).\ndef format_dataset(x, y):\n  return (x, y)\n\ntrain_dataset = list(map(format_dataset, X_train, y_train))\ntest_dataset = list(map(format_dataset, X_test, y_test))\n\n# We also create a dataset with only the textual data in it. This will be used\n# to build our vocabulary later on.\ntextL_dataset = list(map(lambda a:a, x))\n",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print (len(X_train), len(X_test), len(y_train), len(y_test), len(textL_dataset), '\\n',\ntype(X_train), type(X_test), type(y_train), type(y_test), type(textL_dataset))\n",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "900 226 900 226 1126 \n <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'list'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# move our numpy structures into Tensorflow datasets\nDataset = tf.data.Dataset\ntext_dataset = tf.data.Dataset.from_tensor_slices(textL_dataset)\n\nfeatures_dataset = Dataset.from_tensor_slices(X_train)\nlabels_dataset = Dataset.from_tensor_slices(list(y_train))\ntfds_train = Dataset.zip((features_dataset, labels_dataset))\n\nfeatures_test_dataset = Dataset.from_tensor_slices(X_test)\nlabels_test_dataset = Dataset.from_tensor_slices(list(y_test))\ntfds_test = Dataset.zip((features_test_dataset, labels_test_dataset))",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Determine the optimum vocabulary size.  \nThe results for model 2 below showed that varying the vocabulary size was productive, with the optimum size seeming to be between 18000 and 25000.\nWe know there are a lot of junk words in our data, where spaces are missing and words appear only once in a story (ex: we find \"dyma davi d la pajti\" in our text, a French tranliteration of \"Dumas Davy de la Pailleterie\"). \n\nLet's count our words and see what the vocabulary size would be if we removed these."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import re\nimport string\nimport statistics\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import Counter\ndef CleanUpPunctuation(pattern, rep, input_data):\n  lowercase = input_data.lower()\n  s = pattern.sub(lambda m: rep[re.escape(m.group(0))], lowercase)\n  return s\n\n\n#rep = {\"condition1\": \"\", \"condition2\": \"text\"} # define desired replacements here\nrep =  {re.escape(s):\"\" for i,s in enumerate(string.punctuation)}\n# use these three lines to do the replacement\npattern = re.compile(\"|\".join(rep.keys()))\n\nv = list(x)\ncnt = Counter()\nstorylength = []\nunique = []\nvocab = set()\nfor a in v:\n    u = set()\n    a = CleanUpPunctuation(pattern, rep, a)\n    # split returns a list of words delimited by sequences of whitespace (including tabs, newlines, etc, like re's \\s) \n    alist = a.split()\n    storylength.append(len(alist))\n    u = u.union((alist))\n    vocab = vocab.union(alist)\n    unique.append (len(u))\n    for word in alist:\n        cnt[word] += 1\nprint (\"Average story length: {} Minimum story length: {} Maximum story length: {} Standard deviation: {} Total words, counting dupes: {} Total unique words (duped in stories): {}\" \\\n       .format(statistics.mean(storylength), min(storylength), max(storylength), statistics.stdev(storylength), sum(storylength), sum(unique)))\nplt.hist(storylength, 100)",
            "execution_count": 24,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Average story length: 328.6305506216696 Minimum story length: 27 Maximum story length: 14722 Standard deviation: 624.980898772334 Total words, counting dupes: 370038 Total unique words (duped in stories): 189069\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 24,
                    "data": {
                        "text/plain": "(array([552., 248., 168.,  57.,  33.,  16.,   6.,  11.,   3.,   6.,   4.,\n          4.,   1.,   2.,   1.,   1.,   2.,   0.,   1.,   1.,   0.,   0.,\n          0.,   0.,   1.,   1.,   0.,   1.,   2.,   1.,   1.,   0.,   0.,\n          0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          1.]),\n array([   27.  ,   173.95,   320.9 ,   467.85,   614.8 ,   761.75,\n          908.7 ,  1055.65,  1202.6 ,  1349.55,  1496.5 ,  1643.45,\n         1790.4 ,  1937.35,  2084.3 ,  2231.25,  2378.2 ,  2525.15,\n         2672.1 ,  2819.05,  2966.  ,  3112.95,  3259.9 ,  3406.85,\n         3553.8 ,  3700.75,  3847.7 ,  3994.65,  4141.6 ,  4288.55,\n         4435.5 ,  4582.45,  4729.4 ,  4876.35,  5023.3 ,  5170.25,\n         5317.2 ,  5464.15,  5611.1 ,  5758.05,  5905.  ,  6051.95,\n         6198.9 ,  6345.85,  6492.8 ,  6639.75,  6786.7 ,  6933.65,\n         7080.6 ,  7227.55,  7374.5 ,  7521.45,  7668.4 ,  7815.35,\n         7962.3 ,  8109.25,  8256.2 ,  8403.15,  8550.1 ,  8697.05,\n         8844.  ,  8990.95,  9137.9 ,  9284.85,  9431.8 ,  9578.75,\n         9725.7 ,  9872.65, 10019.6 , 10166.55, 10313.5 , 10460.45,\n        10607.4 , 10754.35, 10901.3 , 11048.25, 11195.2 , 11342.15,\n        11489.1 , 11636.05, 11783.  , 11929.95, 12076.9 , 12223.85,\n        12370.8 , 12517.75, 12664.7 , 12811.65, 12958.6 , 13105.55,\n        13252.5 , 13399.45, 13546.4 , 13693.35, 13840.3 , 13987.25,\n        14134.2 , 14281.15, 14428.1 , 14575.05, 14722.  ]),\n <a list of 100 Patch objects>)"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 432x288 with 1 Axes>",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# we have 26884 unique words in our vocabulary\nlen(cnt)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "26884"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "cnt.most_common()[:200:-1]",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "[('planetcom', 1),\n ('indianas', 1),\n ('growsandstates', 1),\n ('biology', 1),\n ('crosses', 1),\n ('sameness', 1),\n ('socio', 1),\n ('indoctrinated', 1),\n ('indoctrination', 1),\n ('psychologists', 1),\n ('molestation', 1),\n ('desensitize', 1),\n ('exam', 1),\n ('correctness', 1),\n ('inclusiveness', 1),\n ('eviscerate', 1),\n ('federally', 1),\n ('passages', 1),\n ('coreapproved', 1),\n ('excelling', 1),\n ('denominator', 1),\n ('assailed', 1),\n ('buttocks', 1),\n ('insert', 1),\n ('pornographic', 1),\n ('dildos', 1),\n ('brook', 1),\n ('stony', 1),\n ('choking', 1),\n ('16yearold', 1),\n ('instinctively', 1),\n ('16yearolds', 1),\n ('patchogue', 1),\n ('42pound', 1),\n ('pix11', 1),\n ('newsday', 1),\n ('euthanized', 1),\n ('sociable', 1),\n ('reunions', 1),\n ('apollo', 1),\n ('armstrongs', 1),\n ('ops', 1),\n ('antarcticas', 1),\n ('spacewalked', 1),\n ('coordinate', 1),\n ('onsite', 1),\n ('deserts', 1),\n ('admunsenscott', 1),\n ('evacuating', 1),\n ('nsf', 1),\n ('precautionary', 1),\n ('amundsenscott', 1),\n ('christchurch', 1),\n ('deploying', 1),\n ('bloomington', 1),\n ('jostled', 1),\n ('freeway', 1),\n ('uic', 1),\n ('calmly', 1),\n ('protester', 1),\n ('kansas', 1),\n ('plummer', 1),\n ('inducing', 1),\n ('jumper', 1),\n ('dimassimo', 1),\n ('buffer', 1),\n ('dayton', 1),\n ('interruptions', 1),\n ('0126', 1),\n ('9977516', 1),\n ('weirdand', 1),\n ('seperatevideos', 1),\n ('wholevid', 1),\n ('videowhere', 1),\n ('seperate', 1),\n ('vid', 1),\n ('22463777', 1),\n ('0134', 1),\n ('13922282', 1),\n ('thati', 1),\n ('0121', 1),\n ('anhedonic', 1),\n ('0115', 1),\n ('clonecloning', 1),\n ('cloning', 1),\n ('clone', 1),\n ('0415', 1),\n ('0408', 1),\n ('fuckheres', 1),\n ('0354', 1),\n ('6231580', 1),\n ('0352', 1),\n ('0351', 1),\n ('0330', 1),\n ('0326', 1),\n ('0323', 1),\n ('fuckhit', 1),\n ('perpetually', 1),\n ('equillibrium', 1),\n ('vector', 1),\n ('octahedral', 1),\n ('medium', 1),\n ('infinitely', 1),\n ('vibrating', 1),\n ('allpermeating', 1),\n ('energymass', 1),\n ('9999999', 1),\n ('0317', 1),\n ('19031228', 1),\n ('0241', 1),\n ('setbackif', 1),\n ('publicfor', 1),\n ('0113', 1),\n ('11214940', 1),\n ('0110', 1),\n ('stepforded', 1),\n ('0105', 1),\n ('swamp', 1),\n ('marksmanship', 1),\n ('trifecta', 1),\n ('boiling', 1),\n ('rises', 1),\n ('11foot', 1),\n ('hauling', 1),\n ('hubcap', 1),\n ('acidic', 1),\n ('digestive', 1),\n ('swallowed', 1),\n ('sniffing', 1),\n ('dangling', 1),\n ('45degree', 1),\n ('rope', 1),\n ('nylon', 1),\n ('willow', 1),\n ('12foot', 1),\n ('gum', 1),\n ('dreamily', 1),\n ('dripping', 1),\n ('mathernes', 1),\n ('nesting', 1),\n ('ibis', 1),\n ('heron', 1),\n ('egrets', 1),\n ('processors', 1),\n ('antifur', 1),\n ('outboards', 1),\n ('bayous', 1),\n ('louisianas', 1),\n ('geological', 1),\n ('harvests', 1),\n ('wetland', 1),\n ('1700acre', 1),\n ('conservationist', 1),\n ('inclined', 1),\n ('upcloseandpersonal', 1),\n ('edam', 1),\n ('mascarpone', 1),\n ('stoneground', 1),\n ('garlic', 1),\n ('celery', 1),\n ('braised', 1),\n ('cutlets', 1),\n ('panfried', 1),\n ('grillades', 1),\n ('inedible', 1),\n ('liver', 1),\n ('kingfish', 1),\n ('babyback', 1),\n ('crocodile', 1),\n ('sourced', 1),\n ('sustainably', 1),\n ('lowfat', 1),\n ('highprotein', 1),\n ('thibodaux', 1),\n ('boating', 1),\n ('chefs', 1),\n ('fishermen', 1),\n ('sara', 1),\n ('octopus', 1),\n ('supertender', 1),\n ('resembled', 1),\n ('crackled', 1),\n ('flavor', 1),\n ('spicy', 1),\n ('fragrant', 1),\n ('paste', 1),\n ('gochujang', 1),\n ('soy', 1),\n ('coconut', 1),\n ('slathered', 1),\n ('tenderloin', 1),\n ('piquant', 1),\n ('tucked', 1),\n ('normalizing', 1),\n ('vanishing', 1),\n ('ofin', 1),\n ('overpopulated', 1),\n ('recoveredwith', 1),\n ('theand', 1),\n ('thespecies', 1),\n ('fluctuating', 1),\n ('untouched', 1),\n ('thehas', 1),\n ('coral', 1),\n ('lighthouse', 1),\n ('freeways', 1),\n ('dodger', 1),\n ('roofs', 1),\n ('backyards', 1),\n ('ebiz', 1),\n ('personnels', 1),\n ('kiram', 1),\n ('ulema', 1),\n ('disobeys', 1),\n ('khaalee', 1),\n ('ul', 1),\n ('rab', 1),\n ('printer', 1),\n ('1204', 1),\n ('apr', 1),\n ('riyadh', 1),\n ('saalim', 1),\n ('9k', 1),\n ('sinow', 1),\n ('pictwittercoml1gtg3puqp', 1),\n ('20499', 1),\n ('passionately', 1),\n ('nominating', 1),\n ('writein', 1),\n ('20500', 1),\n ('whaling', 1),\n ('fin', 1),\n ('activates', 1),\n ('smell', 1),\n ('cuddly', 1),\n ('hurried', 1),\n ('berserk', 1),\n ('perth', 1),\n ('eugene', 1),\n ('21year', 1),\n ('motivating', 1),\n ('consolidation', 1),\n ('acquisition', 1),\n ('pike', 1),\n ('freemasonry', 1),\n ('scottish', 1),\n ('molay', 1),\n ('1307', 1),\n ('priory', 1),\n ('crusades', 1),\n ('templar', 1),\n ('fortunes', 1),\n ('amassing', 1),\n ('198991', 1),\n ('bolshevik', 1),\n ('spanishamerican', 1),\n ('machinations', 1),\n ('instigated', 1),\n ('depressionrecessions', 1),\n ('upheavals', 1),\n ('nwos', 1),\n ('colby', 1),\n ('borda', 1),\n ('moro', 1),\n ('aldo', 1),\n ('bhutto', 1),\n ('dole', 1),\n ('groomed', 1),\n ('exerted', 1),\n ('cooperative', 1),\n ('g7g8', 1),\n ('strata', 1),\n ('controls', 1),\n ('saxecoburggotha', 1),\n ('descendants', 1),\n ('multinational', 1),\n ('cartels', 1),\n ('pharmaceutical', 1),\n ('barons', 1),\n ('puppeteers', 1),\n ('atrocities', 1),\n ('loaning', 1),\n ('machiavellian', 1),\n ('traffickers', 1),\n ('kla', 1),\n ('infrequently', 1),\n ('kadaffi', 1),\n ('milosevic', 1),\n ('hussein', 1),\n ('saddam', 1),\n ('demonized', 1),\n ('liberators', 1),\n ('maneuvered', 1),\n ('impinge', 1),\n ('contrivance', 1),\n ('skilful', 1),\n ('operandi', 1),\n ('modus', 1),\n ('tweak', 1),\n ('recast', 1),\n ('manipulators', 1),\n ('publics', 1),\n ('gauging', 1),\n ('scripted', 1),\n ('solicits', 1),\n ('terminates', 1),\n ('bloodlines', 1),\n ('fritz', 1),\n ('deprogrammer', 1),\n ('brainwashed', 1),\n ('boggling', 1),\n ('firearms', 1),\n ('outlaws', 1),\n ('starved', 1),\n ('subservient', 1),\n ('servants', 1),\n ('famines', 1),\n ('feudal', 1),\n ('selfselect', 1),\n ('oligarchists', 1),\n ('hereditary', 1),\n ('nonelected', 1),\n ('oneunit', 1),\n ('summarizes', 1),\n ('meticulous', 1),\n ('laudable', 1),\n ('loosely', 1),\n ('thirds', 1),\n ('conquest', 1),\n ('warburg', 1),\n ('nationalistic', 1),\n ('nobility', 1),\n ('wealthiest', 1),\n ('echelons', 1),\n ('geneticallyrelated', 1),\n ('generic', 1),\n ('educateyourselforg', 1),\n ('adachi', 1),\n ('romano', 1),\n ('impotent', 1),\n ('abusers', 1),\n ('uns', 1),\n ('slayings', 1),\n ('federalism', 1),\n ('notion', 1),\n ('shatters', 1),\n ('breadth', 1),\n ('video31158785', 1),\n ('lzndn', 1),\n ('zoning', 1),\n ('astorino', 1),\n ('furthering', 1),\n ('affirmatively', 1),\n ('wideranging', 1),\n ('niceties', 1),\n ('fergusonmo54', 1),\n ('miamifl1259', 1),\n ('angelesca10000', 1),\n ('chicagoil11944', 1),\n ('governmentcitystatepolice', 1),\n ('sourceamericans', 1),\n ('titleus', 1),\n ('substantive', 1),\n ('incorporate', 1),\n ('agencywide', 1),\n ('npds', 1),\n ('npd', 1),\n ('77page', 1),\n ('answerable', 1),\n ('bureaucrats', 1),\n ('deadlines', 1),\n ('onbody', 1),\n ('constitutes', 1),\n ('lifezette', 1),\n ('nationalization', 1),\n ('muchfeared', 1),\n ('giulianistyle', 1),\n ('onerous', 1),\n ('widereaching', 1),\n ('municipality', 1),\n ('municipalities', 1),\n ('ferguson', 1),\n ('littleknown', 1),\n ('tailend', 1),\n ('kai', 1),\n ('maina', 1),\n ('rapporteur', 1),\n ('adstrue', 1),\n ('ipq16hto', 1),\n ('lzjwplayer', 1),\n ('18000plus', 1),\n ('federalization', 1),\n ('org', 1),\n ('polizette', 1),\n ('uzbekistan', 1),\n ('emirates', 1),\n ('uganda', 1),\n ('tobago', 1),\n ('trinidad', 1),\n ('tanzania', 1),\n ('swaziland', 1),\n ('sudan', 1),\n ('lanka', 1),\n ('sri', 1),\n ('slovenia', 1),\n ('slovakia', 1),\n ('serbia', 1),\n ('grenadines', 1),\n ('philippines', 1),\n ('panama', 1),\n ('oman', 1),\n ('niger', 1),\n ('nicaragua', 1),\n ('caledonia', 1),\n ('mozambique', 1),\n ('moldova', 1),\n ('mayotte', 1),\n ('mauritius', 1),\n ('maldives', 1),\n ('macedonia', 1),\n ('luxembourg', 1),\n ('latvia', 1),\n ('kuwait', 1),\n ('kazakhstan', 1),\n ('indonesia', 1),\n ('hungary', 1),\n ('honduras', 1),\n ('guatemala', 1),\n ('greenland', 1),\n ('ghana', 1),\n ('polynesia', 1),\n ('finland', 1),\n ('micronesia', 1),\n ('federated', 1),\n ('falkland', 1),\n ('estonia', 1),\n ('salvador', 1),\n ('ecuador', 1),\n ('dominica', 1),\n ('czechia', 1),\n ('croatia', 1),\n ('rica', 1),\n ('burma', 1),\n ('bulgaria', 1),\n ('herzegovina', 1),\n ('belarus', 1),\n ('bangladesh', 1),\n ('bahrain', 1),\n ('azerbaijan', 1),\n ('austria', 1),\n ('aruba', 1),\n ('armenia', 1),\n ('angola', 1),\n ('andorra', 1),\n ('albania', 1),\n ('vivid', 1),\n ('filth', 1),\n ('towels', 1),\n ('airflow', 1),\n ('driers', 1),\n ('secondly', 1),\n ('pictwittercomoulltftq1a', 1),\n ('passageway', 1),\n ('unconsciously', 1),\n ('germinfested', 1),\n ('pictwittercomvpibnjwa0l', 1),\n ('multiply', 1),\n ('exiting', 1),\n ('occupant', 1),\n ('stalls', 1),\n ('germaphobic', 1),\n ('freelance', 1),\n ('designcrowd', 1),\n ('crowdsourcing', 1),\n ('miscast', 1),\n ('roundups', 1),\n ('leaderboard', 1),\n ('creatives', 1),\n ('illustration', 1),\n ('humour', 1),\n ('tutorials', 1),\n ('hybrids', 1),\n ('statues', 1),\n ('zombies', 1),\n ('imaginative', 1),\n ('trove', 1),\n ('worth1000s', 1),\n ('worth1000', 1),\n ('designcrowdcom', 1),\n ('unearthed', 1),\n ('pd', 1),\n ('archrival', 1),\n ('livelihood', 1),\n ('screwing', 1),\n ('pranksters', 1),\n ('vermont', 1),\n ('farva', 1),\n ('shrinking', 1),\n ('dreadlocks', 1),\n ('hasten', 1),\n ('splattered', 1),\n ('subtlety', 1),\n ('seventies', 1),\n ('counterculture', 1),\n ('goodnight', 1),\n ('pierced', 1),\n ('ouuuch', 1),\n ('converse', 1),\n ('rehearsing', 1),\n ('ambushed', 1),\n ('gravity', 1),\n ('rastaman', 1),\n ('jamaican', 1),\n ('brokers', 1),\n ('quo', 1),\n ('1976', 1),\n ('jeopardy', 1),\n ('bobs', 1),\n ('hippy', 1),\n ('longhaired', 1),\n ('assholes', 1),\n ('assassination', 1),\n ('assassinating', 1),\n ('unto', 1),\n ('unconventional', 1),\n ('marksman', 1),\n ('clearances', 1),\n ('toplevel', 1),\n ('livestoday', 1),\n ('opponentsshes', 1),\n ('inxs', 1),\n ('humorous', 1),\n ('meateaters', 1),\n ('jab', 1),\n ('goodhumored', 1),\n ('nealons', 1),\n ('nealon', 1),\n ('dinklages', 1),\n ('dinklage', 1),\n ('eatperiod', 1),\n ('extraordinaire', 1),\n ('jetts', 1),\n ('jett', 1),\n ('mc', 1),\n ('clans', 1),\n ('wutang', 1),\n ('mentalist', 1),\n ('yeoman', 1),\n ('owain', 1),\n ('bassist', 1),\n ('sabbath', 1),\n ('factoryfarming', 1),\n ('animalsand', 1),\n ('enforcers', 1),\n ('nhl', 1),\n ('fenwick', 1),\n ('takeshita', 1),\n ('lipkind', 1),\n ('nora', 1),\n ('lowcalorie', 1),\n ('asterisked', 1),\n ('font', 1),\n ('revisions', 1),\n ('consumerist', 1),\n ('kale', 1),\n ('revise', 1),\n ('casings', 1),\n ('crustaceans', 1),\n ('implementations', 1),\n ('civilian', 1),\n ('clenches', 1),\n ('unclenches', 1),\n ('imagines', 1),\n ('clinically', 1),\n ('bodys', 1),\n ('repeat', 1),\n ('joints', 1),\n ('encases', 1),\n ('skeletonlike', 1),\n ('wearable', 1),\n ('kilograms', 1),\n ('manyfold', 1),\n ('tass', 1),\n ('impulses', 1),\n ('mindcontrolled', 1),\n ('reutlinger', 1),\n ('clich', 1),\n ('leloir', 1),\n ('maurice', 1),\n ('schopp27', 1),\n ('objectives', 1),\n ('dalexandre', 1),\n ('amis', 1),\n ('socit', 1),\n ('decaux', 1),\n ('alain', 1),\n ('himself2526', 1),\n ('1862', 1),\n ('napoli', 1),\n ('di', 1),\n ('borboni', 1),\n ('bourbons', 1),\n ('nouvelles', 1),\n ('astrakan', 1),\n ('czarist', 1),\n ('1833', 1),\n ('corricolo', 1),\n ('calabria', 1),\n ('aeolian', 1),\n ('speronare', 1),\n ('napolinaples', 1),\n ('troy', 1),\n ('troie', 1),\n ('nouvelle', 1),\n ('cadiz', 1),\n ('cadix', 1),\n ('suisse', 1),\n ('encyclopaedia', 1),\n ('lyrique', 1),\n ('renamed', 1),\n ('adolphe', 1),\n ('opra', 1),\n ('1852', 1),\n ('mogador3', 1),\n ('clste', 1),\n ('honorchampion', 1),\n ('nesle', 1),\n ('saracen', 1),\n ('vassaux', 1),\n ('ses', 1),\n ('chez', 1),\n ('vassals', 1),\n ('antony', 1),\n ('mademoiselle', 1),\n ('comdiefranaise', 1),\n ('hernani', 1),\n ('hugos', 1),\n ('preceding', 1),\n ('cour', 1),\n ('sa', 1),\n ('dramatist', 1),\n ('chevalier', 1),\n ('18531855', 1),\n ('comtesse', 1),\n ('bastille', 1),\n ('storming', 1),\n ('pitou', 1),\n ('18491850', 1),\n ('reine', 1),\n ('collier', 1),\n ('mesmerists', 1),\n ('lifejoseph', 1),\n ('cagliostro', 1),\n ('18461848', 1),\n ('mdecin', 1),\n ('dun', 1),\n ('mmoires', 1),\n ('1347', 1),\n ('15591560', 1),\n ('horoscope', 1),\n ('deux', 1),\n ('medici', 1),\n ('1589', 1),\n ('1328', 1),\n ('mahalin', 1),\n ('aramis', 1),\n ('porthos', 1),\n ('valliere', 1),\n ('louise', 1),\n ('tard', 1),\n ('aprs', 1),\n ('vingt', 1),\n ('mousquetaires', 1),\n ('trois', 1),\n ('proscrit1873', 1),\n ('outlaw', 1),\n ('richelieu', 1),\n ('manuscript', 1),\n ('quarterblack', 1),\n ('slaves', 1),\n ('naming', 1),\n ('candie', 1),\n ('slaveholder', 1),\n ('unchained', 1),\n ('django', 1),\n ('200814', 1),\n ('lempire', 1),\n ('salut', 1),\n ('saintshermine', 1),\n ('languages14', 1),\n ('phbus', 1),\n ('ditions', 1),\n ('story14', 1),\n ('1869', 1),\n ('serially', 1),\n ('trafalgar', 1),\n ('zola2223', 1),\n ('mile', 1),\n ('correcting', 1),\n ('pantheon', 1),\n ('reinterment', 1),\n ('dream22', 1),\n ('castleswith', 1),\n ('battlefields', 1),\n ('panthon11', 1),\n ('costumed', 1),\n ('caisson', 1),\n ('cloth', 1),\n ('buried314', 1),\n ('luminaries', 1),\n ('mausoleum', 1),\n ('reinterred', 1),\n ('honouring', 1),\n ('bicentennial', 1),\n ('dumas2021', 1),\n ('bibliography', 1),\n ('death19', 1),\n ('libraries', 1),\n ('auckland', 1),\n ('editions', 1),\n ('sheets', 1),\n ('3350', 1),\n ('pharmacist', 1),\n ('whangarei', 1),\n ('dunedin', 1),\n ('18741953', 1),\n ('postal', 1),\n ('honorchampion3', 1),\n ('fr', 1),\n ('rginald', 1),\n ('museumcitation', 1),\n ('mtro', 1),\n ('works3', 1),\n ('reappraisal', 1),\n ('fashions', 1),\n ('francoprussian', 1),\n ('1870', 1),\n ('children14', 1),\n ('mistresses', 1),\n ('success18', 1),\n ('savanne', 1),\n ('soldout', 1),\n ('mazeppa', 1),\n ('emlie', 1),\n ('micallaclliejosephalisabeth', 1),\n ('18031875', 1),\n ('krelsamer', 1),\n ('liaisons', 1),\n ('1811185917', 1),\n ('ferrand', 1),\n ('margueritejosphine', 1),\n ('ferrier', 1),\n ('ida', 1),\n ('ends1516', 1),\n ('greatgrandfather', 1),\n ('negro', 1),\n ('mulatto', 1),\n ('colonialism', 1),\n ('indipendente', 1),\n ('tbilisi', 1),\n ('kazan', 1),\n ('creditors', 1),\n ('disapproved', 1),\n ('lifetime3', 1),\n ('generosity', 1),\n ('portmarly', 1),\n ('mistresses14', 1),\n ('lavishly', 1),\n ('insolvent', 1),\n ('byline1213', 1),\n ('authorial', 1),\n ('plots', 1),\n ('understood12', 1),\n ('auguste', 1),\n ('corsican', 1),\n ('czars', 1),\n ('czar', 1),\n ('decembrist', 1),\n ('grisiers', 1),\n ('augustin', 1),\n ('desrues', 1),\n ('ludwig', 1),\n ('murderers', 1),\n ('borgia', 1),\n ('lucrezia', 1),\n ('cesare', 1),\n ('guerre', 1),\n ('cenci', 1),\n ('beatrice', 1),\n ('eightvolume', 1),\n ('staffed', 1),\n ('rewrote', 1),\n ('marketer4', 1),\n ('industrialise', 1),\n ('disgruntled', 1),\n ('riots', 1),\n ('sporadic', 1),\n ('unsettled', 1),\n ('mid1830s', 1),\n ('christine', 1),\n ('adult11', 1),\n ('grandmothers', 1),\n ('orlanscitation', 1),\n ('palais', 1),\n ('restoration', 1),\n ('1822', 1),\n ('widowed', 1),\n ('1806', 1),\n ('taranto', 1),\n ('generalinchief', 1),\n ('revolutionary', 1),\n ('distinction', 1),\n ('army10', 1),\n ('afroantilles', 1),\n ('came789', 1),\n ('thomasalexandres', 1),\n ('afrocaribbean', 1),\n ('artillery', 1),\n ('commissaire', 1),\n ('gnral', 1),\n ('marquis', 1),\n ('innkeeper', 1),\n ('labouret', 1),\n ('lisabeth', 1),\n ('17976', 1),\n ('1796', 1),\n ('louisealexandrine', 1),\n ('1794', 1),\n ('picardy', 1),\n ('himself5', 1),\n ('windmill', 1),\n ('egotistical', 1),\n ('amusing', 1),\n ('delightfully', 1),\n ('largehearted', 1),\n ('phillips', 1),\n ('assisted', 1),\n ('wedlock', 1),\n ('twentiethcentury', 1),\n ('illegitimate', 1),\n ('frenchmen', 1),\n ('acquire', 1),\n ('illustrious', 1),\n ('descent4', 1),\n ('pages3', 1),\n ('20052', 1),\n ('serials', 1),\n ('18701', 1),\n ('pajti', 1),\n ('davi', 1),\n ('alksd', 1),\n ('panther', 1),\n ('uniting', 1),\n ('artifacts', 1),\n ('receivership', 1),\n ('usda', 1),\n ('40000', 1),\n ('timely', 1),\n ('timeliness', 1),\n ('chronically', 1),\n ('decreasing', 1),\n ('ranking', 1),\n ('bangor', 1),\n ('mismanaging', 1),\n ('constitutionally', 1),\n ('highskilled', 1),\n ('immigrationreform', 1),\n ('pathway', 1),\n ('rflorida', 1),\n ('marco', 1),\n ('bludgeon', 1),\n ('proimmigration', 1),\n ('rhetoric', 1),\n ('practicality', 1),\n ('brushed', 1),\n ('backlog', 1),\n ('regabusiness', 1),\n ('blodget', 1),\n ('gowdy', 1),\n ('exhume', 1),\n ('fairfax', 1),\n ('steer', 1),\n ('florio', 1),\n ('prevented', 1),\n ('injustices', 1),\n ('kneeling', 1),\n ('blackballed', 1),\n ('unsigned', 1),\n ('qb', 1),\n ('nflcaliber', 1),\n ('freeagency', 1),\n ('grievance', 1),\n ('kaepernicks', 1),\n ('rosters', 1),\n ('straightkirkus', 1),\n ('frighteningly', 1),\n ('decried', 1),\n ('vanishedthe', 1),\n ('greedy', 1),\n ('sly', 1),\n ('bloodsportlevel', 1),\n ('enthralling', 1),\n ('skillful', 1),\n ('headlinegrabbing', 1),\n ('unneeded', 1),\n ('resorting', 1),\n ('youarethere', 1),\n ('crises', 1),\n ('beset', 1),\n ('undeniably', 1),\n ('jobthe', 1),\n ('wellqualified', 1),\n ('cynicism', 1),\n ('cattiness', 1),\n ('muckraking', 1),\n ('ironies', 1),\n ('dysfunctionthe', 1),\n ('littered', 1),\n ('systemthe', 1),\n ('karmic', 1),\n ('sharpened', 1),\n ('talons', 1),\n ('justly', 1),\n ('yearthen', 1),\n ('clevelands', 1),\n ('165372', 1),\n ('tightly', 1),\n ('laundering', 1),\n ('minimizes', 1),\n ('inconveniently', 1),\n ('progressively', 1),\n ('153', 1),\n ('368', 1),\n ('discontinued', 1),\n ('24th', 1),\n ('peal', 1),\n ('entails', 1),\n ('hillarys', 1),\n ('pj', 1),\n ('hintergrund', 1),\n ('ernsten', 1),\n ('einen', 1),\n ('sondern', 1),\n ('aprilpointe', 1),\n ('lustige', 1),\n ('keine', 1),\n ('gibt', 1),\n ('daher', 1),\n ('knnten', 1),\n ('werden', 1),\n ('schnell', 1),\n ('sehr', 1),\n ('plastikverschmutzung', 1),\n ('durch', 1),\n ('gefahren', 1),\n ('solche', 1),\n ('welt', 1),\n ('einer', 1),\n ('wir', 1),\n ('doch', 1),\n ('bildmontage', 1),\n ('eine', 1),\n ('sich', 1),\n ('handelt', 1),\n ('eis', 1),\n ('ihrer', 1),\n ('freudig', 1),\n ('weiterhin', 1),\n ('und', 1),\n ('wohlauf', 1),\n ('sind', 1),\n ('pinguine', 1),\n ('gefilmten', 1),\n ('alle', 1),\n ('echt', 1),\n ('nicht', 1),\n ('glcklicherweise', 1),\n ('ist', 1),\n ('plastikmll', 1),\n ('eselspinguine', 1),\n ('entdeckung', 1),\n ('unsere', 1),\n ('queries', 1),\n ('intrepid', 1),\n ('sexism', 1),\n ('sloppiness', 1),\n ('besieged', 1),\n ('mettle', 1),\n ('confines', 1),\n ('chummier', 1),\n ('highstakes', 1),\n ('onemonth', 1),\n ('dependents', 1),\n ('ut', 1),\n ('az', 1),\n ('ks', 1),\n ('sc', 1),\n ('wy', 1),\n ('underemployed', 1),\n ('revising', 1),\n ...]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# we have 12,139 words that only appear once in any article.\n# 26,884 - 12,139 = \ncntd = dict(cnt)\nsort_orders = sorted(cntd.items(), key=lambda x: x[1], reverse=False)\nsingles = []\nfor i in sort_orders:\n    if i[1] ==1:\n        singles.append(i[0])\nlen(singles)",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "12139"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Vocabulary Size\nOf the 26,884 words in our articles, we have 12,139 words used once, leaving 14,745 words found in more than one article.  Any analysis that depends on finding the same word multiple articles will not find any of these 12,139 words, so from that standpoint they are just noise.\n\nHowever, if we can find a way to analyze on a scope broader than word or ngram repetition, such as sentence structure, or by inferring parts of speech, even words that appear once words might be useful.\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### (Below is just kept as a note to myself on how to find a record based on its key value.)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "i = dfNews.index.get_loc('biz01legit')\nprint (i)\n#dfNews.iloc[i:i+2]\ndfNews['text'][159]\n\n",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "159\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "'Alex Jones Apologizes for Promoting \\'Pizzagate\\' Hoax  Alex Jones  a prominent conspiracy theorist and the host of a popular right-wing radio show  has apologized for helping to spread and promote the hoax known as Pizzagate. The admission on Friday by Mr. Jones  the host of \"The Alex Jones Show\" and the operator of the website Infowars  was striking. In addition to promoting the Pizzagate conspiracy theory  he has contended that the Sept. 11 attacks were inside jobs carried out by the United States government and that the 2012 shooting at Sandy Hook Elementary School in Newtown  Conn.  was a hoax concocted by those hostile to the Second Amendment. The Pizzagate theory  which posited with no evidence that top Democratic officials were involved with a satanic child pornography ring centered around Comet Ping Pong  a pizza restaurant in Washington  D.C.  grew in online forums before making its way to more visible venues  including Mr. Jones\\'s show. And its prominence after the election drew attention to the proliferation of false and misleading news  much of it politically charged  that circulated on platforms like Facebook  Twitter and YouTube.'"
                    },
                    "metadata": {}
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}