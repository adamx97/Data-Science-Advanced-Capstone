{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Feature Engineering--True and False News\nThe final step of feature engineering is to tokenize the text of the stories.  The raw data sequence of characters cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n\nI considered doing this using discrete Scikit-Learn modules, but the recently Tensorflow 2.1 adds support for a TextVectorization layer, and 2.3 adds experiment support for the new Keras Preprocessing Layers API. These layers allow you to package preprocessing logic inside the model for easier deployment \u2014 allowing the model to take raw strings, images, or rows from a table as input.  This module also includes a \n\nThe processing of each sample contains the following steps:\n\n1. Standardize each sample.  Lowercase all words and strip punctuation. \n\n2. Split each sample into substrings (usually words).\n\n3. Recombine substrings into tokens (usually ngrams). Options here include determining how many words to include in each token.  Text classification tasks typically  consider tokens of 1 or 2 works, but we may experiment with more than that.\n\n4. Index tokens (associate a unique int value with each token).\n\n5. Transform each sample using this index, either into a vector of ints or a dense float vector.  This layer includes the ability to set the length of the resulting vector, either truncating or padding the vector with zeroes so it will fit the size of our input layer.  It also has several output modes, including tf-idf which is weighting algorithm based on the frequency of words found on the dataset.\n\nFrom the Scikit-learn documentation: \n> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.  In a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\nIn order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf\u2013idf transform.\n\nWe will consider varying output modes as we go forward. \n\nSince the TextVectorization layer will allow us to convert the text of our stories to integrer tensors, so there is not much for us to do with feature engineering.  \n\n## Vocabulary Size\nThe default TextVectorization settings will retain all words found as part of our vocabulary.  In experimenting, we found some value in altering the vocabulary size.  \n\nBelow is the code we used to count words and find words that only appeared once, with the hypothesis that any word that appeared in only one article could not inform decisions on any other articles. The code and some further discussion is found below. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade numpy\n!pip install --upgrade pandas\n\n# we want tensorflow 2.3\n!pip install --upgrade tensorflow  ",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting numpy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5MB 9.3MB/s eta 0:00:01\n\u001b[31mERROR: tensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mERROR: autoai-libs 1.10.5 has requirement pandas>=0.24.2, but you'll have pandas 0.24.1 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Found existing installation: numpy 1.15.4\n    Uninstalling numpy-1.15.4:\n      Successfully uninstalled numpy-1.15.4\nSuccessfully installed numpy-1.19.1\nCollecting pandas\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/c6/9ac4ae44c24c787a1738e5fb34dd987ada6533de5905a041aa6d5bea4553/pandas-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.5MB 8.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (1.19.1)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2018.9)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\nInstalling collected packages: pandas\n  Found existing installation: pandas 0.24.1\n    Uninstalling pandas-0.24.1:\n      Successfully uninstalled pandas-0.24.1\nSuccessfully installed pandas-1.1.1\nCollecting tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/ae/0b08f53498417914f2274cc3b5576d2b83179b0cbb209457d0fde0152174/tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 320.4MB 45.9MB/s eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u258f                          | 52.1MB 45.3MB/s eta 0:00:06\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 283.6MB 578kB/s eta 0:01:04\n\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 24.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (3.11.2)\nCollecting scipy==1.4.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1MB 26.0MB/s eta 0:00:01    |\u2588                               | 798kB 26.0MB/s eta 0:00:01\n\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 20.6MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.7.0)\nRequirement already satisfied, skipping upgrade: wheel>=0.26 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.32.3)\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.16.1)\nCollecting numpy<1.19.0,>=1.16.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.1MB 25.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.11.1)\nCollecting tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 460kB 40.0MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.12.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.12.0)\nCollecting astunparse==1.6.3 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\nCollecting gast==0.3.3 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\nCollecting h5py<2.11.0,>=2.10.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 39.9MB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard<3,>=2.3.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 24.5MB/s eta 0:00:01\n\u001b[?25hCollecting google-pasta>=0.1.8 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 20.6MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/Python36/lib/python3.6/site-packages (from protobuf>=3.9.2->tensorflow) (40.8.0)\nCollecting google-auth<2,>=1.6.3 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/79/4c59796bb02535aee5e5d2e2c5e16008aaf48903c2ec2ff566a2774bb3e0/google_auth-1.20.1-py2.py3-none-any.whl (91kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 19.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.21.0)\nCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 788kB 37.1MB/s eta 0:00:01\n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\nRequirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.14.1)\nRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.0.1)\nCollecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 19.0MB/s eta 0:00:01\n\u001b[?25hCollecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 36.7MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\nRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.1)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nCollecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 26.1MB/s eta 0:00:01\n\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 40.1MB/s eta 0:00:01\n\u001b[31mERROR: tensorboard 2.3.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.16.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorboard 2.3.0 has requirement setuptools>=41.0.0, but you'll have setuptools 40.8.0 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, opt-einsum, scipy, keras-preprocessing, tensorflow-estimator, astunparse, gast, h5py, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, google-pasta, tensorflow\n  Found existing installation: numpy 1.19.1\n    Uninstalling numpy-1.19.1:\n      Successfully uninstalled numpy-1.19.1\n  Found existing installation: scipy 1.2.0\n    Uninstalling scipy-1.2.0:\n      Successfully uninstalled scipy-1.2.0\n  Found existing installation: Keras-Preprocessing 1.0.5\n    Uninstalling Keras-Preprocessing-1.0.5:\n      Successfully uninstalled Keras-Preprocessing-1.0.5\n  Found existing installation: tensorflow-estimator 1.13.0\n    Uninstalling tensorflow-estimator-1.13.0:\n      Successfully uninstalled tensorflow-estimator-1.13.0\n  Found existing installation: astunparse 1.6.2\n    Uninstalling astunparse-1.6.2:\n      Successfully uninstalled astunparse-1.6.2\n  Found existing installation: gast 0.2.2\n    Uninstalling gast-0.2.2:\n      Successfully uninstalled gast-0.2.2\n  Found existing installation: h5py 2.9.0\n    Uninstalling h5py-2.9.0:\n      Successfully uninstalled h5py-2.9.0\n  Found existing installation: tensorflow 1.13.1\n    Uninstalling tensorflow-1.13.1:\n      Successfully uninstalled tensorflow-1.13.1\nSuccessfully installed astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.20.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 h5py-2.10.0 keras-preprocessing-1.1.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf\nprint(\"Tensorflow version: \", tf.__version__)\nif not tf.__version__ == '2.3.0':\n    raise ValueError('please upgrade to TensorFlow 2.3, or restart your Kernel (Kernel->Restart & Clear Output)')",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Tensorflow version:  2.3.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom time import time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport string\nimport re\n\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Get our data\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_news = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-32e8ee67-397c-4ff1-b69b-543172331f43',\n    'IBM_API_KEY_ID': 'Rx4FR4JSAueCnnIsoevsgYgOsuh8LCXtbkFpFpC0EmVU',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'advanceddatasciencecapstone-donotdelete-pr-tqabpnbxebk8rm',\n    'FILE': 'dfTrueFalseNews.pkl'\n}\n\ndef download_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.download_file(Bucket=credentials['BUCKET'],Key=key,Filename=local_file_name)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print('File Downloaded')\n\ndef upload_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.upload_file(Filename=local_file_name, Bucket=credentials['BUCKET'],Key=key)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print(' File Uploaded')\n        \ndfNews = download_file_cos(credentials_news, \"dfTrueFalseNews.pkl\", \"dfTrueFalseNews.pkl\")",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNews = pd.read_pickle('dfTrueFalseNews.pkl')\n#dfNews['truthvalue'] = pd.Categorical(dfNews['truthvalue'])\n\nprint (dfNews.shape, dfNews.columns, '\\n',  dfNews.dtypes)",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(1129, 3) Index(['text', 'source', 'truthvalue'], dtype='object') \n text          object\nsource        object\ntruthvalue    object\ndtype: object\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNews.head()",
            "execution_count": 28,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 28,
                    "data": {
                        "text/plain": "                                                            text  \\\ntech010legit   AT&T pulls ads from YouTube, other Google site...   \ntech008legit   Are Autonomous Cars Ready to Go It Alone?  Tra...   \npolit24legit   Back Channel to Trump: Loyal Aide in Trump Tow...   \nedu32legit     Students Experiment With Drones for 4-H Nation...   \nsports01legit   Basketball 'bible' auction sets sports memora...   \n\n                          source truthvalue  \ntech010legit   MihalceaNewsLegit          1  \ntech008legit   MihalceaNewsLegit          1  \npolit24legit   MihalceaNewsLegit          1  \nedu32legit     MihalceaNewsLegit          1  \nsports01legit  MihalceaNewsLegit          1  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech010legit</th>\n      <td>AT&amp;T pulls ads from YouTube, other Google site...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>tech008legit</th>\n      <td>Are Autonomous Cars Ready to Go It Alone?  Tra...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>polit24legit</th>\n      <td>Back Channel to Trump: Loyal Aide in Trump Tow...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>edu32legit</th>\n      <td>Students Experiment With Drones for 4-H Nation...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>sports01legit</th>\n      <td>Basketball 'bible' auction sets sports memora...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "x = dfNews['text'].values\ny = dfNews['truthvalue'].values\nprint(type(x), type(y))",
            "execution_count": 29,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n# Once we have our handles, we format the datasets in a Keras-fit compatible\n# format: a tuple of the form (text_data, label).\ndef format_dataset(x, y):\n  return (x, y)\n\ntrain_dataset = list(map(format_dataset, X_train, y_train))\ntest_dataset = list(map(format_dataset, X_test, y_test))\n\n# We also create a dataset with only the textual data in it. This will be used\n# to build our vocabulary later on.\ntextL_dataset = list(map(lambda a:a, x))\n",
            "execution_count": 30,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print (len(X_train), len(X_test), len(y_train), len(y_test), len(text_dataset), '\\n',\ntype(X_train), type(X_test), type(y_train), type(y_test), type(text_dataset))\n",
            "execution_count": 31,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "903 226 903 226 1129 \n <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# move our numpy structures into Tensorflow datasets\nDataset = tf.data.Dataset\ntext_dataset = tf.data.Dataset.from_tensor_slices(textL_dataset)\n\nfeatures_dataset = Dataset.from_tensor_slices(X_train)\nlabels_dataset = Dataset.from_tensor_slices(list(y_train))\ntfds_train = Dataset.zip((features_dataset, labels_dataset))\n\nfeatures_test_dataset = Dataset.from_tensor_slices(X_test)\nlabels_test_dataset = Dataset.from_tensor_slices(list(y_test))\ntfds_test = Dataset.zip((features_test_dataset, labels_test_dataset))",
            "execution_count": 32,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Try to determine the optimum vocabulary size.  \nThe results for model 2 below showed that varying the vocabulary size was productive, with the optimum size seeming to be between 18000 and 25000.\nWe know there are a lot of junk words in our data, where spaces are missing and words appear only once in a story (ex: we find \"dyma davi d la pajti\" in our text, a French tranliteration of \"Dumas Davy de la Pailleterie\"). \n\nLet's count our words and see what the vocabulary size would be if we removed these."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import re\nimport string\nfrom collections import Counter\ndef CleanUpPunctuation(pattern, rep, input_data):\n  lowercase = input_data.lower()\n  s = pattern.sub(lambda m: rep[re.escape(m.group(0))], lowercase)\n  return s\n\n\n#rep = {\"condition1\": \"\", \"condition2\": \"text\"} # define desired replacements here\nrep =  {re.escape(s):\"\" for i,s in enumerate(string.punctuation)}\n# use these three lines to do the replacement\npattern = re.compile(\"|\".join(rep.keys()))\n\nv = list(x)\ncnt = Counter()\nfor a in v:\n    a = CleanUpPunctuation(pattern, rep, a)\n    # split returns a list of words delimited by sequences of whitespace (including tabs, newlines, etc, like re's \\s) \n    alist = a.split()\n    for word in alist:\n        cnt[word] += 1\n",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# we have 26884 unique words in our vocabulary\nlen(cnt)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "26884"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "cnt.most_common()[:200:-1]",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "[('planetcom', 1),\n ('indianas', 1),\n ('growsandstates', 1),\n ('biology', 1),\n ('crosses', 1),\n ('sameness', 1),\n ('socio', 1),\n ('indoctrinated', 1),\n ('indoctrination', 1),\n ('psychologists', 1),\n ('molestation', 1),\n ('desensitize', 1),\n ('exam', 1),\n ('correctness', 1),\n ('inclusiveness', 1),\n ('eviscerate', 1),\n ('federally', 1),\n ('passages', 1),\n ('coreapproved', 1),\n ('excelling', 1),\n ('denominator', 1),\n ('assailed', 1),\n ('buttocks', 1),\n ('insert', 1),\n ('pornographic', 1),\n ('dildos', 1),\n ('brook', 1),\n ('stony', 1),\n ('choking', 1),\n ('16yearold', 1),\n ('instinctively', 1),\n ('16yearolds', 1),\n ('patchogue', 1),\n ('42pound', 1),\n ('pix11', 1),\n ('newsday', 1),\n ('euthanized', 1),\n ('sociable', 1),\n ('reunions', 1),\n ('apollo', 1),\n ('armstrongs', 1),\n ('ops', 1),\n ('antarcticas', 1),\n ('spacewalked', 1),\n ('coordinate', 1),\n ('onsite', 1),\n ('deserts', 1),\n ('admunsenscott', 1),\n ('evacuating', 1),\n ('nsf', 1),\n ('precautionary', 1),\n ('amundsenscott', 1),\n ('christchurch', 1),\n ('deploying', 1),\n ('bloomington', 1),\n ('jostled', 1),\n ('freeway', 1),\n ('uic', 1),\n ('calmly', 1),\n ('protester', 1),\n ('kansas', 1),\n ('plummer', 1),\n ('inducing', 1),\n ('jumper', 1),\n ('dimassimo', 1),\n ('buffer', 1),\n ('dayton', 1),\n ('interruptions', 1),\n ('0126', 1),\n ('9977516', 1),\n ('weirdand', 1),\n ('seperatevideos', 1),\n ('wholevid', 1),\n ('videowhere', 1),\n ('seperate', 1),\n ('vid', 1),\n ('22463777', 1),\n ('0134', 1),\n ('13922282', 1),\n ('thati', 1),\n ('0121', 1),\n ('anhedonic', 1),\n ('0115', 1),\n ('clonecloning', 1),\n ('cloning', 1),\n ('clone', 1),\n ('0415', 1),\n ('0408', 1),\n ('fuckheres', 1),\n ('0354', 1),\n ('6231580', 1),\n ('0352', 1),\n ('0351', 1),\n ('0330', 1),\n ('0326', 1),\n ('0323', 1),\n ('fuckhit', 1),\n ('perpetually', 1),\n ('equillibrium', 1),\n ('vector', 1),\n ('octahedral', 1),\n ('medium', 1),\n ('infinitely', 1),\n ('vibrating', 1),\n ('allpermeating', 1),\n ('energymass', 1),\n ('9999999', 1),\n ('0317', 1),\n ('19031228', 1),\n ('0241', 1),\n ('setbackif', 1),\n ('publicfor', 1),\n ('0113', 1),\n ('11214940', 1),\n ('0110', 1),\n ('stepforded', 1),\n ('0105', 1),\n ('swamp', 1),\n ('marksmanship', 1),\n ('trifecta', 1),\n ('boiling', 1),\n ('rises', 1),\n ('11foot', 1),\n ('hauling', 1),\n ('hubcap', 1),\n ('acidic', 1),\n ('digestive', 1),\n ('swallowed', 1),\n ('sniffing', 1),\n ('dangling', 1),\n ('45degree', 1),\n ('rope', 1),\n ('nylon', 1),\n ('willow', 1),\n ('12foot', 1),\n ('gum', 1),\n ('dreamily', 1),\n ('dripping', 1),\n ('mathernes', 1),\n ('nesting', 1),\n ('ibis', 1),\n ('heron', 1),\n ('egrets', 1),\n ('processors', 1),\n ('antifur', 1),\n ('outboards', 1),\n ('bayous', 1),\n ('louisianas', 1),\n ('geological', 1),\n ('harvests', 1),\n ('wetland', 1),\n ('1700acre', 1),\n ('conservationist', 1),\n ('inclined', 1),\n ('upcloseandpersonal', 1),\n ('edam', 1),\n ('mascarpone', 1),\n ('stoneground', 1),\n ('garlic', 1),\n ('celery', 1),\n ('braised', 1),\n ('cutlets', 1),\n ('panfried', 1),\n ('grillades', 1),\n ('inedible', 1),\n ('liver', 1),\n ('kingfish', 1),\n ('babyback', 1),\n ('crocodile', 1),\n ('sourced', 1),\n ('sustainably', 1),\n ('lowfat', 1),\n ('highprotein', 1),\n ('thibodaux', 1),\n ('boating', 1),\n ('chefs', 1),\n ('fishermen', 1),\n ('sara', 1),\n ('octopus', 1),\n ('supertender', 1),\n ('resembled', 1),\n ('crackled', 1),\n ('flavor', 1),\n ('spicy', 1),\n ('fragrant', 1),\n ('paste', 1),\n ('gochujang', 1),\n ('soy', 1),\n ('coconut', 1),\n ('slathered', 1),\n ('tenderloin', 1),\n ('piquant', 1),\n ('tucked', 1),\n ('normalizing', 1),\n ('vanishing', 1),\n ('ofin', 1),\n ('overpopulated', 1),\n ('recoveredwith', 1),\n ('theand', 1),\n ('thespecies', 1),\n ('fluctuating', 1),\n ('untouched', 1),\n ('thehas', 1),\n ('coral', 1),\n ('lighthouse', 1),\n ('freeways', 1),\n ('dodger', 1),\n ('roofs', 1),\n ('backyards', 1),\n ('ebiz', 1),\n ('personnels', 1),\n ('kiram', 1),\n ('ulema', 1),\n ('disobeys', 1),\n ('khaalee', 1),\n ('ul', 1),\n ('rab', 1),\n ('printer', 1),\n ('1204', 1),\n ('apr', 1),\n ('riyadh', 1),\n ('saalim', 1),\n ('9k', 1),\n ('sinow', 1),\n ('pictwittercoml1gtg3puqp', 1),\n ('20499', 1),\n ('passionately', 1),\n ('nominating', 1),\n ('writein', 1),\n ('20500', 1),\n ('whaling', 1),\n ('fin', 1),\n ('activates', 1),\n ('smell', 1),\n ('cuddly', 1),\n ('hurried', 1),\n ('berserk', 1),\n ('perth', 1),\n ('eugene', 1),\n ('21year', 1),\n ('motivating', 1),\n ('consolidation', 1),\n ('acquisition', 1),\n ('pike', 1),\n ('freemasonry', 1),\n ('scottish', 1),\n ('molay', 1),\n ('1307', 1),\n ('priory', 1),\n ('crusades', 1),\n ('templar', 1),\n ('fortunes', 1),\n ('amassing', 1),\n ('198991', 1),\n ('bolshevik', 1),\n ('spanishamerican', 1),\n ('machinations', 1),\n ('instigated', 1),\n ('depressionrecessions', 1),\n ('upheavals', 1),\n ('nwos', 1),\n ('colby', 1),\n ('borda', 1),\n ('moro', 1),\n ('aldo', 1),\n ('bhutto', 1),\n ('dole', 1),\n ('groomed', 1),\n ('exerted', 1),\n ('cooperative', 1),\n ('g7g8', 1),\n ('strata', 1),\n ('controls', 1),\n ('saxecoburggotha', 1),\n ('descendants', 1),\n ('multinational', 1),\n ('cartels', 1),\n ('pharmaceutical', 1),\n ('barons', 1),\n ('puppeteers', 1),\n ('atrocities', 1),\n ('loaning', 1),\n ('machiavellian', 1),\n ('traffickers', 1),\n ('kla', 1),\n ('infrequently', 1),\n ('kadaffi', 1),\n ('milosevic', 1),\n ('hussein', 1),\n ('saddam', 1),\n ('demonized', 1),\n ('liberators', 1),\n ('maneuvered', 1),\n ('impinge', 1),\n ('contrivance', 1),\n ('skilful', 1),\n ('operandi', 1),\n ('modus', 1),\n ('tweak', 1),\n ('recast', 1),\n ('manipulators', 1),\n ('publics', 1),\n ('gauging', 1),\n ('scripted', 1),\n ('solicits', 1),\n ('terminates', 1),\n ('bloodlines', 1),\n ('fritz', 1),\n ('deprogrammer', 1),\n ('brainwashed', 1),\n ('boggling', 1),\n ('firearms', 1),\n ('outlaws', 1),\n ('starved', 1),\n ('subservient', 1),\n ('servants', 1),\n ('famines', 1),\n ('feudal', 1),\n ('selfselect', 1),\n ('oligarchists', 1),\n ('hereditary', 1),\n ('nonelected', 1),\n ('oneunit', 1),\n ('summarizes', 1),\n ('meticulous', 1),\n ('laudable', 1),\n ('loosely', 1),\n ('thirds', 1),\n ('conquest', 1),\n ('warburg', 1),\n ('nationalistic', 1),\n ('nobility', 1),\n ('wealthiest', 1),\n ('echelons', 1),\n ('geneticallyrelated', 1),\n ('generic', 1),\n ('educateyourselforg', 1),\n ('adachi', 1),\n ('romano', 1),\n ('impotent', 1),\n ('abusers', 1),\n ('uns', 1),\n ('slayings', 1),\n ('federalism', 1),\n ('notion', 1),\n ('shatters', 1),\n ('breadth', 1),\n ('video31158785', 1),\n ('lzndn', 1),\n ('zoning', 1),\n ('astorino', 1),\n ('furthering', 1),\n ('affirmatively', 1),\n ('wideranging', 1),\n ('niceties', 1),\n ('fergusonmo54', 1),\n ('miamifl1259', 1),\n ('angelesca10000', 1),\n ('chicagoil11944', 1),\n ('governmentcitystatepolice', 1),\n ('sourceamericans', 1),\n ('titleus', 1),\n ('substantive', 1),\n ('incorporate', 1),\n ('agencywide', 1),\n ('npds', 1),\n ('npd', 1),\n ('77page', 1),\n ('answerable', 1),\n ('bureaucrats', 1),\n ('deadlines', 1),\n ('onbody', 1),\n ('constitutes', 1),\n ('lifezette', 1),\n ('nationalization', 1),\n ('muchfeared', 1),\n ('giulianistyle', 1),\n ('onerous', 1),\n ('widereaching', 1),\n ('municipality', 1),\n ('municipalities', 1),\n ('ferguson', 1),\n ('littleknown', 1),\n ('tailend', 1),\n ('kai', 1),\n ('maina', 1),\n ('rapporteur', 1),\n ('adstrue', 1),\n ('ipq16hto', 1),\n ('lzjwplayer', 1),\n ('18000plus', 1),\n ('federalization', 1),\n ('org', 1),\n ('polizette', 1),\n ('uzbekistan', 1),\n ('emirates', 1),\n ('uganda', 1),\n ('tobago', 1),\n ('trinidad', 1),\n ('tanzania', 1),\n ('swaziland', 1),\n ('sudan', 1),\n ('lanka', 1),\n ('sri', 1),\n ('slovenia', 1),\n ('slovakia', 1),\n ('serbia', 1),\n ('grenadines', 1),\n ('philippines', 1),\n ('panama', 1),\n ('oman', 1),\n ('niger', 1),\n ('nicaragua', 1),\n ('caledonia', 1),\n ('mozambique', 1),\n ('moldova', 1),\n ('mayotte', 1),\n ('mauritius', 1),\n ('maldives', 1),\n ('macedonia', 1),\n ('luxembourg', 1),\n ('latvia', 1),\n ('kuwait', 1),\n ('kazakhstan', 1),\n ('indonesia', 1),\n ('hungary', 1),\n ('honduras', 1),\n ('guatemala', 1),\n ('greenland', 1),\n ('ghana', 1),\n ('polynesia', 1),\n ('finland', 1),\n ('micronesia', 1),\n ('federated', 1),\n ('falkland', 1),\n ('estonia', 1),\n ('salvador', 1),\n ('ecuador', 1),\n ('dominica', 1),\n ('czechia', 1),\n ('croatia', 1),\n ('rica', 1),\n ('burma', 1),\n ('bulgaria', 1),\n ('herzegovina', 1),\n ('belarus', 1),\n ('bangladesh', 1),\n ('bahrain', 1),\n ('azerbaijan', 1),\n ('austria', 1),\n ('aruba', 1),\n ('armenia', 1),\n ('angola', 1),\n ('andorra', 1),\n ('albania', 1),\n ('vivid', 1),\n ('filth', 1),\n ('towels', 1),\n ('airflow', 1),\n ('driers', 1),\n ('secondly', 1),\n ('pictwittercomoulltftq1a', 1),\n ('passageway', 1),\n ('unconsciously', 1),\n ('germinfested', 1),\n ('pictwittercomvpibnjwa0l', 1),\n ('multiply', 1),\n ('exiting', 1),\n ('occupant', 1),\n ('stalls', 1),\n ('germaphobic', 1),\n ('freelance', 1),\n ('designcrowd', 1),\n ('crowdsourcing', 1),\n ('miscast', 1),\n ('roundups', 1),\n ('leaderboard', 1),\n ('creatives', 1),\n ('illustration', 1),\n ('humour', 1),\n ('tutorials', 1),\n ('hybrids', 1),\n ('statues', 1),\n ('zombies', 1),\n ('imaginative', 1),\n ('trove', 1),\n ('worth1000s', 1),\n ('worth1000', 1),\n ('designcrowdcom', 1),\n ('unearthed', 1),\n ('pd', 1),\n ('archrival', 1),\n ('livelihood', 1),\n ('screwing', 1),\n ('pranksters', 1),\n ('vermont', 1),\n ('farva', 1),\n ('shrinking', 1),\n ('dreadlocks', 1),\n ('hasten', 1),\n ('splattered', 1),\n ('subtlety', 1),\n ('seventies', 1),\n ('counterculture', 1),\n ('goodnight', 1),\n ('pierced', 1),\n ('ouuuch', 1),\n ('converse', 1),\n ('rehearsing', 1),\n ('ambushed', 1),\n ('gravity', 1),\n ('rastaman', 1),\n ('jamaican', 1),\n ('brokers', 1),\n ('quo', 1),\n ('1976', 1),\n ('jeopardy', 1),\n ('bobs', 1),\n ('hippy', 1),\n ('longhaired', 1),\n ('assholes', 1),\n ('assassination', 1),\n ('assassinating', 1),\n ('unto', 1),\n ('unconventional', 1),\n ('marksman', 1),\n ('clearances', 1),\n ('toplevel', 1),\n ('livestoday', 1),\n ('opponentsshes', 1),\n ('inxs', 1),\n ('humorous', 1),\n ('meateaters', 1),\n ('jab', 1),\n ('goodhumored', 1),\n ('nealons', 1),\n ('nealon', 1),\n ('dinklages', 1),\n ('dinklage', 1),\n ('eatperiod', 1),\n ('extraordinaire', 1),\n ('jetts', 1),\n ('jett', 1),\n ('mc', 1),\n ('clans', 1),\n ('wutang', 1),\n ('mentalist', 1),\n ('yeoman', 1),\n ('owain', 1),\n ('bassist', 1),\n ('sabbath', 1),\n ('factoryfarming', 1),\n ('animalsand', 1),\n ('enforcers', 1),\n ('nhl', 1),\n ('fenwick', 1),\n ('takeshita', 1),\n ('lipkind', 1),\n ('nora', 1),\n ('lowcalorie', 1),\n ('asterisked', 1),\n ('font', 1),\n ('revisions', 1),\n ('consumerist', 1),\n ('kale', 1),\n ('revise', 1),\n ('casings', 1),\n ('crustaceans', 1),\n ('implementations', 1),\n ('civilian', 1),\n ('clenches', 1),\n ('unclenches', 1),\n ('imagines', 1),\n ('clinically', 1),\n ('bodys', 1),\n ('repeat', 1),\n ('joints', 1),\n ('encases', 1),\n ('skeletonlike', 1),\n ('wearable', 1),\n ('kilograms', 1),\n ('manyfold', 1),\n ('tass', 1),\n ('impulses', 1),\n ('mindcontrolled', 1),\n ('reutlinger', 1),\n ('clich', 1),\n ('leloir', 1),\n ('maurice', 1),\n ('schopp27', 1),\n ('objectives', 1),\n ('dalexandre', 1),\n ('amis', 1),\n ('socit', 1),\n ('decaux', 1),\n ('alain', 1),\n ('himself2526', 1),\n ('1862', 1),\n ('napoli', 1),\n ('di', 1),\n ('borboni', 1),\n ('bourbons', 1),\n ('nouvelles', 1),\n ('astrakan', 1),\n ('czarist', 1),\n ('1833', 1),\n ('corricolo', 1),\n ('calabria', 1),\n ('aeolian', 1),\n ('speronare', 1),\n ('napolinaples', 1),\n ('troy', 1),\n ('troie', 1),\n ('nouvelle', 1),\n ('cadiz', 1),\n ('cadix', 1),\n ('suisse', 1),\n ('encyclopaedia', 1),\n ('lyrique', 1),\n ('renamed', 1),\n ('adolphe', 1),\n ('opra', 1),\n ('1852', 1),\n ('mogador3', 1),\n ('clste', 1),\n ('honorchampion', 1),\n ('nesle', 1),\n ('saracen', 1),\n ('vassaux', 1),\n ('ses', 1),\n ('chez', 1),\n ('vassals', 1),\n ('antony', 1),\n ('mademoiselle', 1),\n ('comdiefranaise', 1),\n ('hernani', 1),\n ('hugos', 1),\n ('preceding', 1),\n ('cour', 1),\n ('sa', 1),\n ('dramatist', 1),\n ('chevalier', 1),\n ('18531855', 1),\n ('comtesse', 1),\n ('bastille', 1),\n ('storming', 1),\n ('pitou', 1),\n ('18491850', 1),\n ('reine', 1),\n ('collier', 1),\n ('mesmerists', 1),\n ('lifejoseph', 1),\n ('cagliostro', 1),\n ('18461848', 1),\n ('mdecin', 1),\n ('dun', 1),\n ('mmoires', 1),\n ('1347', 1),\n ('15591560', 1),\n ('horoscope', 1),\n ('deux', 1),\n ('medici', 1),\n ('1589', 1),\n ('1328', 1),\n ('mahalin', 1),\n ('aramis', 1),\n ('porthos', 1),\n ('valliere', 1),\n ('louise', 1),\n ('tard', 1),\n ('aprs', 1),\n ('vingt', 1),\n ('mousquetaires', 1),\n ('trois', 1),\n ('proscrit1873', 1),\n ('outlaw', 1),\n ('richelieu', 1),\n ('manuscript', 1),\n ('quarterblack', 1),\n ('slaves', 1),\n ('naming', 1),\n ('candie', 1),\n ('slaveholder', 1),\n ('unchained', 1),\n ('django', 1),\n ('200814', 1),\n ('lempire', 1),\n ('salut', 1),\n ('saintshermine', 1),\n ('languages14', 1),\n ('phbus', 1),\n ('ditions', 1),\n ('story14', 1),\n ('1869', 1),\n ('serially', 1),\n ('trafalgar', 1),\n ('zola2223', 1),\n ('mile', 1),\n ('correcting', 1),\n ('pantheon', 1),\n ('reinterment', 1),\n ('dream22', 1),\n ('castleswith', 1),\n ('battlefields', 1),\n ('panthon11', 1),\n ('costumed', 1),\n ('caisson', 1),\n ('cloth', 1),\n ('buried314', 1),\n ('luminaries', 1),\n ('mausoleum', 1),\n ('reinterred', 1),\n ('honouring', 1),\n ('bicentennial', 1),\n ('dumas2021', 1),\n ('bibliography', 1),\n ('death19', 1),\n ('libraries', 1),\n ('auckland', 1),\n ('editions', 1),\n ('sheets', 1),\n ('3350', 1),\n ('pharmacist', 1),\n ('whangarei', 1),\n ('dunedin', 1),\n ('18741953', 1),\n ('postal', 1),\n ('honorchampion3', 1),\n ('fr', 1),\n ('rginald', 1),\n ('museumcitation', 1),\n ('mtro', 1),\n ('works3', 1),\n ('reappraisal', 1),\n ('fashions', 1),\n ('francoprussian', 1),\n ('1870', 1),\n ('children14', 1),\n ('mistresses', 1),\n ('success18', 1),\n ('savanne', 1),\n ('soldout', 1),\n ('mazeppa', 1),\n ('emlie', 1),\n ('micallaclliejosephalisabeth', 1),\n ('18031875', 1),\n ('krelsamer', 1),\n ('liaisons', 1),\n ('1811185917', 1),\n ('ferrand', 1),\n ('margueritejosphine', 1),\n ('ferrier', 1),\n ('ida', 1),\n ('ends1516', 1),\n ('greatgrandfather', 1),\n ('negro', 1),\n ('mulatto', 1),\n ('colonialism', 1),\n ('indipendente', 1),\n ('tbilisi', 1),\n ('kazan', 1),\n ('creditors', 1),\n ('disapproved', 1),\n ('lifetime3', 1),\n ('generosity', 1),\n ('portmarly', 1),\n ('mistresses14', 1),\n ('lavishly', 1),\n ('insolvent', 1),\n ('byline1213', 1),\n ('authorial', 1),\n ('plots', 1),\n ('understood12', 1),\n ('auguste', 1),\n ('corsican', 1),\n ('czars', 1),\n ('czar', 1),\n ('decembrist', 1),\n ('grisiers', 1),\n ('augustin', 1),\n ('desrues', 1),\n ('ludwig', 1),\n ('murderers', 1),\n ('borgia', 1),\n ('lucrezia', 1),\n ('cesare', 1),\n ('guerre', 1),\n ('cenci', 1),\n ('beatrice', 1),\n ('eightvolume', 1),\n ('staffed', 1),\n ('rewrote', 1),\n ('marketer4', 1),\n ('industrialise', 1),\n ('disgruntled', 1),\n ('riots', 1),\n ('sporadic', 1),\n ('unsettled', 1),\n ('mid1830s', 1),\n ('christine', 1),\n ('adult11', 1),\n ('grandmothers', 1),\n ('orlanscitation', 1),\n ('palais', 1),\n ('restoration', 1),\n ('1822', 1),\n ('widowed', 1),\n ('1806', 1),\n ('taranto', 1),\n ('generalinchief', 1),\n ('revolutionary', 1),\n ('distinction', 1),\n ('army10', 1),\n ('afroantilles', 1),\n ('came789', 1),\n ('thomasalexandres', 1),\n ('afrocaribbean', 1),\n ('artillery', 1),\n ('commissaire', 1),\n ('gnral', 1),\n ('marquis', 1),\n ('innkeeper', 1),\n ('labouret', 1),\n ('lisabeth', 1),\n ('17976', 1),\n ('1796', 1),\n ('louisealexandrine', 1),\n ('1794', 1),\n ('picardy', 1),\n ('himself5', 1),\n ('windmill', 1),\n ('egotistical', 1),\n ('amusing', 1),\n ('delightfully', 1),\n ('largehearted', 1),\n ('phillips', 1),\n ('assisted', 1),\n ('wedlock', 1),\n ('twentiethcentury', 1),\n ('illegitimate', 1),\n ('frenchmen', 1),\n ('acquire', 1),\n ('illustrious', 1),\n ('descent4', 1),\n ('pages3', 1),\n ('20052', 1),\n ('serials', 1),\n ('18701', 1),\n ('pajti', 1),\n ('davi', 1),\n ('alksd', 1),\n ('panther', 1),\n ('uniting', 1),\n ('artifacts', 1),\n ('receivership', 1),\n ('usda', 1),\n ('40000', 1),\n ('timely', 1),\n ('timeliness', 1),\n ('chronically', 1),\n ('decreasing', 1),\n ('ranking', 1),\n ('bangor', 1),\n ('mismanaging', 1),\n ('constitutionally', 1),\n ('highskilled', 1),\n ('immigrationreform', 1),\n ('pathway', 1),\n ('rflorida', 1),\n ('marco', 1),\n ('bludgeon', 1),\n ('proimmigration', 1),\n ('rhetoric', 1),\n ('practicality', 1),\n ('brushed', 1),\n ('backlog', 1),\n ('regabusiness', 1),\n ('blodget', 1),\n ('gowdy', 1),\n ('exhume', 1),\n ('fairfax', 1),\n ('steer', 1),\n ('florio', 1),\n ('prevented', 1),\n ('injustices', 1),\n ('kneeling', 1),\n ('blackballed', 1),\n ('unsigned', 1),\n ('qb', 1),\n ('nflcaliber', 1),\n ('freeagency', 1),\n ('grievance', 1),\n ('kaepernicks', 1),\n ('rosters', 1),\n ('straightkirkus', 1),\n ('frighteningly', 1),\n ('decried', 1),\n ('vanishedthe', 1),\n ('greedy', 1),\n ('sly', 1),\n ('bloodsportlevel', 1),\n ('enthralling', 1),\n ('skillful', 1),\n ('headlinegrabbing', 1),\n ('unneeded', 1),\n ('resorting', 1),\n ('youarethere', 1),\n ('crises', 1),\n ('beset', 1),\n ('undeniably', 1),\n ('jobthe', 1),\n ('wellqualified', 1),\n ('cynicism', 1),\n ('cattiness', 1),\n ('muckraking', 1),\n ('ironies', 1),\n ('dysfunctionthe', 1),\n ('littered', 1),\n ('systemthe', 1),\n ('karmic', 1),\n ('sharpened', 1),\n ('talons', 1),\n ('justly', 1),\n ('yearthen', 1),\n ('clevelands', 1),\n ('165372', 1),\n ('tightly', 1),\n ('laundering', 1),\n ('minimizes', 1),\n ('inconveniently', 1),\n ('progressively', 1),\n ('153', 1),\n ('368', 1),\n ('discontinued', 1),\n ('24th', 1),\n ('peal', 1),\n ('entails', 1),\n ('hillarys', 1),\n ('pj', 1),\n ('hintergrund', 1),\n ('ernsten', 1),\n ('einen', 1),\n ('sondern', 1),\n ('aprilpointe', 1),\n ('lustige', 1),\n ('keine', 1),\n ('gibt', 1),\n ('daher', 1),\n ('knnten', 1),\n ('werden', 1),\n ('schnell', 1),\n ('sehr', 1),\n ('plastikverschmutzung', 1),\n ('durch', 1),\n ('gefahren', 1),\n ('solche', 1),\n ('welt', 1),\n ('einer', 1),\n ('wir', 1),\n ('doch', 1),\n ('bildmontage', 1),\n ('eine', 1),\n ('sich', 1),\n ('handelt', 1),\n ('eis', 1),\n ('ihrer', 1),\n ('freudig', 1),\n ('weiterhin', 1),\n ('und', 1),\n ('wohlauf', 1),\n ('sind', 1),\n ('pinguine', 1),\n ('gefilmten', 1),\n ('alle', 1),\n ('echt', 1),\n ('nicht', 1),\n ('glcklicherweise', 1),\n ('ist', 1),\n ('plastikmll', 1),\n ('eselspinguine', 1),\n ('entdeckung', 1),\n ('unsere', 1),\n ('queries', 1),\n ('intrepid', 1),\n ('sexism', 1),\n ('sloppiness', 1),\n ('besieged', 1),\n ('mettle', 1),\n ('confines', 1),\n ('chummier', 1),\n ('highstakes', 1),\n ('onemonth', 1),\n ('dependents', 1),\n ('ut', 1),\n ('az', 1),\n ('ks', 1),\n ('sc', 1),\n ('wy', 1),\n ('underemployed', 1),\n ('revising', 1),\n ...]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# we have 12,139 words that only appear once in any article.\n# 26,884 - 12,139 = \ncntd = dict(cnt)\nsort_orders = sorted(cntd.items(), key=lambda x: x[1], reverse=False)\nsingles = []\nfor i in sort_orders:\n    if i[1] ==1:\n        singles.append(i[0])\nlen(singles)",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "12139"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Vocabulary Size\nOf the 26,884 words in our articles, we have 12,139 words used once, leaving 14,745 words found in more than one article.  Any analysis that depends on finding the same word multiple articles will not find any of these 12,139 words, so from that standpoint they are just noise.\n\nHowever, if we can find a way to analyze on a scope broader than word or ngram repetition, such as sentence structure, or by inferring parts of speech, even words that appear once words might be useful.\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### (Below is just kept as a note to myself on how to find a record based on its key value.)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "i = dfNews.index.get_loc('biz01legit')\nprint (i)\n#dfNews.iloc[i:i+2]\ndfNews['text'][159]\n\n",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "159\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "'Alex Jones Apologizes for Promoting \\'Pizzagate\\' Hoax  Alex Jones  a prominent conspiracy theorist and the host of a popular right-wing radio show  has apologized for helping to spread and promote the hoax known as Pizzagate. The admission on Friday by Mr. Jones  the host of \"The Alex Jones Show\" and the operator of the website Infowars  was striking. In addition to promoting the Pizzagate conspiracy theory  he has contended that the Sept. 11 attacks were inside jobs carried out by the United States government and that the 2012 shooting at Sandy Hook Elementary School in Newtown  Conn.  was a hoax concocted by those hostile to the Second Amendment. The Pizzagate theory  which posited with no evidence that top Democratic officials were involved with a satanic child pornography ring centered around Comet Ping Pong  a pizza restaurant in Washington  D.C.  grew in online forums before making its way to more visible venues  including Mr. Jones\\'s show. And its prominence after the election drew attention to the proliferation of false and misleading news  much of it politically charged  that circulated on platforms like Facebook  Twitter and YouTube.'"
                    },
                    "metadata": {}
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}