{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": [
                "### Binary classification with Non-Deep learning algorithms: Naive Bayes and SVM\n",
                "We will use Naive Bayes and Support Vector machine.\n"
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "!pip install --upgrade numpy\n",
                "!pip install --upgrade pandas\n",
                "!pip install pyspark==2.4.5\n",
                "!pip install -U scikit-learn"
            ],
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.19.2)\nRequirement already up-to-date: pandas in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.1.2)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: numpy>=1.15.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (1.19.2)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2018.9)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\nCollecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |████████████████████████████████| 217.8MB 171kB/s  eta 0:00:01    |████▌                           | 30.3MB 38.4MB/s eta 0:00:05     |████████████▎                   | 83.4MB 30.8MB/s eta 0:00:05     |█████████████████████▉          | 148.3MB 45.8MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |████████████████████████████████| 204kB 34.9MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\nRequirement already up-to-date: scikit-learn in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.23.2)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.19.2)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (0.16.0)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "try:\n",
                "    from pyspark import SparkContext, SparkConf\n",
                "    from pyspark.sql import SparkSession\n",
                "except ImportError as e:\n",
                "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
            ],
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
                "\n",
                "spark = SparkSession \\\n",
                "    .builder \\\n",
                "    .getOrCreate()\n"
            ],
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "%matplotlib inline\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "from pprint import pprint\n",
                "from time import time\n",
                "import logging\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import string\n",
                "import re\n",
                "from datetime import datetime\n",
                "from packaging import version\n",
                "\n",
                "from ibm_botocore.client import Config\n",
                "import ibm_boto3\n",
                "\n",
                "from sklearn.model_selection import train_test_split"
            ],
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "#Get our data from IBM Cloud\n",
                "\n",
                "# @hidden_cell\n",
                "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
                "# You might want to remove those credentials before you share your notebook.\n",
                "credentials_news = {\n",
                "    'IAM_SERVICE_ID': 'iam-ServiceId-32e8ee67-397c-4ff1-b69b-543172331f43',\n",
                "    'IBM_API_KEY_ID': 'Rx4FR4JSAueCnnIsoevsgYgOsuh8LCXtbkFpFpC0EmVU',\n",
                "    #'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
                "    'ENDPOINT':'https://s3-api.us-geo.objectstorage.softlayer.net',\n",
                "    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n",
                "    'BUCKET': 'advanceddatasciencecapstone-donotdelete-pr-tqabpnbxebk8rm',\n",
                "    'FILE': 'dfTrueFalseNews.pkl'\n",
                "}\n",
                "\n",
                "def download_file_cos(credentials,local_file_name,key):  \n",
                "    cos = ibm_boto3.client(service_name='s3',\n",
                "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
                "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
                "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
                "    config=Config(signature_version='oauth'),\n",
                "    endpoint_url=credentials['ENDPOINT'])\n",
                "    try:\n",
                "        res=cos.download_file(Bucket=credentials['BUCKET'],Key=key,Filename=local_file_name)\n",
                "    except Exception as e:\n",
                "        print(Exception, e)\n",
                "    else:\n",
                "        print('File Downloaded')\n",
                "\n",
                "def upload_file_cos(credentials,local_file_name,key):  \n",
                "    cos = ibm_boto3.client(service_name='s3',\n",
                "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
                "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
                "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
                "    config=Config(signature_version='oauth'),\n",
                "    endpoint_url=credentials['ENDPOINT'])\n",
                "    try:\n",
                "        res=cos.upload_file(Filename=local_file_name, Bucket=credentials['BUCKET'],Key=key)\n",
                "    except Exception as e:\n",
                "        print(Exception, e)\n",
                "    else:\n",
                "        print(' File Uploaded')\n",
                "        \n",
                "dfNews = download_file_cos(credentials_news, \"dfTrueFalseNews.pkl\", \"dfTrueFalseNews.pkl\")\n",
                "dfTrueFalseNews_tokenized  = download_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl','dfTrueFalseNews_tokenized.pkl')"
            ],
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\nFile Downloaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "dfNewsTemp = pd.read_pickle('dfTrueFalseNews.pkl')\n",
                "#dfNews['truthvalue'] = pd.Categorical(dfNews['truthvalue'])\n",
                "x = dfNewsTemp['text'].values\n",
                "\n",
                "y = dfNewsTemp['truthvalue'].values\n",
                "\n",
                "print (dfNewsTemp.shape, dfNewsTemp.columns, '\\n', dfNewsTemp.dtypes, type(x), type(y))\n"
            ],
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "(1126, 3) Index(['text', 'source', 'truthvalue'], dtype='object') \n text          object\nsource        object\ntruthvalue    object\ndtype: object <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "### Prepare the Text\n",
                "1. Change all the text to lower case\n",
                "2. Word Tokenization\n",
                "3. Remove Stop words\n",
                "4. Remove Non-alpha text\n",
                "5. Word Lemmatization"
            ]
        },
        {
            "metadata": {
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "from nltk.tokenize import word_tokenize\n",
                "from nltk import pos_tag\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from nltk.stem.porter import *\n",
                "from nltk.corpus import wordnet as wn\n",
                "from collections import defaultdict\n",
                "import nltk\n",
                "nltk.download('punkt')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('averaged_perceptron_tagger')\n",
                "nltk.download('stopwords')\n",
                "\n",
                "# reproduce the same result every time the script is run.\n",
                "np.random.seed(500)"
            ],
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\AdamDavis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\AdamDavis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\AdamDavis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\AdamDavis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# Step 1: Change all the text to lower case.\n",
                "dfNewsTemp['text'] = [entry.lower() for entry in dfNewsTemp['text']]"
            ],
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# Step 2: Tokenization : Each entry will be broken into set of words\n",
                "dfNewsTemp['text']= [word_tokenize(entry) for entry in dfNewsTemp['text']]"
            ],
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# Step 3,4,5: Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
                "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
                "tag_map = defaultdict(lambda : wn.NOUN)\n",
                "tag_map['J'] = wn.ADJ\n",
                "tag_map['V'] = wn.VERB\n",
                "tag_map['R'] = wn.ADV"
            ],
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# Initializing WordNetLemmatizer()\n",
                "word_Lemmatized = WordNetLemmatizer()\n",
                "#dfNewsTemp.loc['text_final'].astype('object')\n",
                "col = []\n",
                "for index,entry in enumerate(dfNewsTemp['text']):\n",
                "    # Declaring Empty List to store the words that follow the rules for this step\n",
                "    Final_words = []\n",
                "    # Initializing WordNetLemmatizer()\n",
                "    #word_Lemmatized = WordNetLemmatizer()\n",
                "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
                "    for word, tag in pos_tag(entry):\n",
                "        # Below condition is to check for Stop words and consider only alphabets\n",
                "        #if word not in stopwords.words('english') and word.isalpha():\n",
                "        if word.isalpha():\n",
                "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
                "            Final_words.append(word_Final)\n",
                "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
                "    \n",
                "    #dfNewsTemp.loc[index,'text_final'] = \" \".join(Final_words)\n",
                "    #dfNewsTemp.loc[index,'text_final'] = Final_words\n",
                "    col.append(\" \".join(Final_words))\n",
                "dfNewsTemp['text_final'] = col\n"
            ],
            "execution_count": 63,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "#dfNewsTemp['text_final'] = col\n",
                "#col[-1]"
            ],
            "execution_count": 23,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# This took a while, so let's save the result.\n",
                "#dfNewsTemp.to_pickle('dfTrueFalseNews_tokenized_nostopwords.pkl', protocol=4 )\n",
                "#dfNewsTemp.to_pickle('dfTrueFalseNews_tokenized.pkl', protocol=4 )\n",
                "\n",
                "# save to our cloud storage \n",
                "#upload_file_cos(credentials_news,'dfTrueFalseNews_tokenized_nostopwords.pkl','dfTrueFalseNews_tokenized_nostopwords.pkl')"
            ],
            "execution_count": 41,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "dfTrueFalseNews_tokenized  = download_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl', 'dfTrueFalseNews_tokenized.pkl')\n",
                "!ls -al"
            ],
            "execution_count": 52,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'download_file_cos' is not defined",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-52-5d73473145f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfTrueFalseNews_tokenized\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdownload_file_cos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials_news\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'dfTrueFalseNews_tokenized.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dfTrueFalseNews_tokenized.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ls -al'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'download_file_cos' is not defined"
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "#dfTrueFalseNews_tokenized = pd.read_pickle('dfTrueFalseNews_tokenized.pkl')\n",
                "dfTrueFalseNews_tokenized = pd.read_pickle('dfTrueFalseNews_tokenized_nostopwords.pkl')\n",
                "\n",
                "dfTrueFalseNews_tokenized.head()"
            ],
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "                                                           text  \\\ntech003legit  [a, google, computer, victorious, over, the, w...   \npolit11legit  [white, house, keeps, up, sanctuary, cities, p...   \nbiz40legit    [why, silicon, valley, is, n't, fighting, to, ...   \nedu10legit    [protesters, disrupt, devos, school, visit, pr...   \ntech038legit  [solar-powered, 'skin, ', could, make, prosthe...   \n\n                         source truthvalue  \\\ntech003legit  MihalceaNewsLegit          1   \npolit11legit  MihalceaNewsLegit          1   \nbiz40legit    MihalceaNewsLegit          1   \nedu10legit    MihalceaNewsLegit          1   \ntech038legit  MihalceaNewsLegit          1   \n\n                                                     text_final  \ntech003legit  google computer victorious world champion satu...  \npolit11legit  white house keep sanctuary city pressure fundi...  \nbiz40legit    silicon valley fight save internet yet think n...  \nedu10legit    protester disrupt devos school visit protester...  \ntech038legit  could make prosthetics real many people try st...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>[a, google, computer, victorious, over, the, w...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>google computer victorious world champion satu...</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>[white, house, keeps, up, sanctuary, cities, p...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>white house keep sanctuary city pressure fundi...</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>[why, silicon, valley, is, n't, fighting, to, ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>silicon valley fight save internet yet think n...</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>[protesters, disrupt, devos, school, visit, pr...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>protester disrupt devos school visit protester...</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>[solar-powered, 'skin, ', could, make, prosthe...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>could make prosthetics real many people try st...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": [
                "from sklearn import model_selection\n",
                "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dfTrueFalseNews_tokenized['text_final'],dfTrueFalseNews_tokenized['truthvalue'],test_size=0.1, stratify=dfTrueFalseNews_tokenized['truthvalue'], random_state=42, shuffle=True)\n",
                "#print(dfTrueFalseNews_tokenized.shape, (dfTrueFalseNews_tokenized['text_final'].shape), (dfTrueFalseNews_tokenized['truthvalue'].shape))\n",
                "#dfTrueFalseNews_tokenized['text_final'][0]"
            ],
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "#corpus = [\" \".join(i) for i in dfTrueFalseNews_tokenized['text_final']]\n",
                "#corpus[1:3]\n",
                "dfTrueFalseNews_tokenized.head()\n",
                "\n",
                "#dfTrueFalseNews_tokenized.dtypes\n",
                "#import sklearn\n",
                "#sklearn.show_versions()"
            ],
            "execution_count": 56,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "                                                           text  \\\ntech003legit  [a, google, computer, victorious, over, the, w...   \npolit11legit  [white, house, keeps, up, sanctuary, cities, p...   \nbiz40legit    [why, silicon, valley, is, n't, fighting, to, ...   \nedu10legit    [protesters, disrupt, devos, school, visit, pr...   \ntech038legit  [solar-powered, 'skin, ', could, make, prosthe...   \n\n                         source truthvalue  \\\ntech003legit  MihalceaNewsLegit          1   \npolit11legit  MihalceaNewsLegit          1   \nbiz40legit    MihalceaNewsLegit          1   \nedu10legit    MihalceaNewsLegit          1   \ntech038legit  MihalceaNewsLegit          1   \n\n                                                     text_final  \ntech003legit  google computer victorious world champion satu...  \npolit11legit  white house keep sanctuary city pressure fundi...  \nbiz40legit    silicon valley fight save internet yet think n...  \nedu10legit    protester disrupt devos school visit protester...  \ntech038legit  could make prosthetics real many people try st...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>[a, google, computer, victorious, over, the, w...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>google computer victorious world champion satu...</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>[white, house, keeps, up, sanctuary, cities, p...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>white house keep sanctuary city pressure fundi...</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>[why, silicon, valley, is, n't, fighting, to, ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>silicon valley fight save internet yet think n...</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>[protesters, disrupt, devos, school, visit, pr...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>protester disrupt devos school visit protester...</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>[solar-powered, 'skin, ', could, make, prosthe...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>could make prosthetics real many people try st...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 56
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# some open source utilities to evaluate a model.\n",
                "# you can see them at: https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch05_Building_Tuning_and_Deploying_Models/model_evaluation_utils.py\n",
                "import importlib\n",
                "#importlib.reload(model_evaluation_utils)\n",
                "import model_evaluation_utils as meu"
            ],
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "### Use Naive Bayes Classifier with ngrams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "ctVectorizer = CountVectorizer(ngram_range=(1,2), min_df=1 )\n",
                "\n",
                "ctVectorizer.fit_transform(dfTrueFalseNews_tokenized['text_final'])\n",
                "\n",
                "Train_X_2ngrams = ctVectorizer.transform(Train_X)\n",
                "Test_X_2ngrams = ctVectorizer.transform(Test_X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Naive Bayes Accuracy Score ->  46.017699115044245\nNaive model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.4602\nPrecision: 0.4594\nRecall: 0.4602\nF1 Score: 0.4589\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.45      0.41      0.43        56\n           0       0.47      0.51      0.49        57\n\n    accuracy                           0.46       113\n   macro avg       0.46      0.46      0.46       113\nweighted avg       0.46      0.46      0.46       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1         23  33\n        0         28  29\n"
                }
            ],
            "source": [
                "from sklearn import naive_bayes\n",
                "from sklearn.metrics import accuracy_score\n",
                "# fit the training dataset on the NB classifier\n",
                "Naive = naive_bayes.MultinomialNB()\n",
                "Train_y_int = Train_Y.astype('int')\n",
                "Test_y_int = Test_Y.astype('int')\n",
                "Naive.fit(Train_X_2ngrams,Train_y_int)# predict the labels on validation dataset\n",
                "#Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
                "predictions_NB = Naive.predict(Test_X_2ngrams)# Use accuracy_score function to get the accuracy\n",
                "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_y_int)* 100)\n",
                "print(\"Naive model params: {}\".format(Naive.get_params()))\n",
                "meu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_NB.tolist())"
            ]
        },
        {
            "source": [
                "### Naive Bayes Classifier with tf-idf vectorizer"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "Tfidf_vect = TfidfVectorizer()\n",
                "\n",
                "Tfidf_vect.fit(dfTrueFalseNews_tokenized['text_final'])\n",
                "\n",
                "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
                "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
            ],
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "from sklearn import naive_bayes\n",
                "from sklearn.metrics import accuracy_score\n",
                "# fit the training dataset on the NB classifier\n",
                "Naive = naive_bayes.MultinomialNB()\n",
                "Train_y_int = Train_Y.astype('int')\n",
                "Test_y_int = Test_Y.astype('int')\n",
                "Naive.fit(Train_X_Tfidf,Train_y_int)# predict the labels on validation dataset\n",
                "#Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
                "predictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
                "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_y_int)* 100)\n",
                "print(\"Naive model params: {}\".format(Naive.get_params()))\n",
                "meu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_NB.tolist())"
            ],
            "execution_count": 71,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Naive Bayes Accuracy Score ->  46.017699115044245\nNaive model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.4602\nPrecision: 0.4584\nRecall: 0.4602\nF1 Score: 0.4563\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.45      0.38      0.41        56\n           0       0.47      0.54      0.50        57\n\n    accuracy                           0.46       113\n   macro avg       0.46      0.46      0.46       113\nweighted avg       0.46      0.46      0.46       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1         21  35\n        0         26  31\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "### Discussion\n",
                "This is a Naive Bayes model using a Bag of Words TF-IDF encoding. \n",
                "\n",
                "The result of 46% accuracy is not very good.  "
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "## Use Support Vector Machine\n",
                "We use a tf-idf embedding."
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "from sklearn import svm\n",
                "# Classifier - Algorithm - SVM\n",
                "# fit the training dataset on the classifier\n",
                "SVM = svm.SVC(C=2.5, kernel='sigmoid', degree=3, gamma='scale')\n",
                "SVM.fit(Train_X_Tfidf,Train_y_int)# predict the labels on validation dataset\n",
                "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
                "print(\"SVM Accuracy Score -> {}\".format(accuracy_score(predictions_SVM, Test_y_int)*100))\n",
                "print(\"SVM model params: {}\".format(Naive.get_params()))\n",
                "meu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_SVM.tolist())"
            ],
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "SVM Accuracy Score -> 59.29203539823009\nSVM model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.5929\nPrecision: 0.6035\nRecall: 0.5929\nF1 Score: 0.5889\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.64      0.49      0.56        59\n           0       0.56      0.70      0.62        54\n\n    accuracy                           0.59       113\n   macro avg       0.60      0.60      0.59       113\nweighted avg       0.60      0.59      0.59       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1         29  30\n        0         16  38\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "### Discussion\n",
                "\n",
                "This is a Support Vector Machine using the same Bag of Words TF-IDF encoding as above and does surprisingly better, with a 59% accuracy rate that rivals others results.\n",
                "\n"
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "### Split the data"
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "# use the dataframe to write a parquet file\n",
                "dfNewsTemp.to_parquet('dfTrueFalseNews_tokenized.parquet')\n",
                "dfNews = spark.read.parquet('dfTrueFalseNews_tokenized.parquet')\n",
                "dfNews.createOrReplaceTempView('dfNews')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dfNewsTemp['text_final'],Corpus['label'],test_size=0.1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": [
                "splits = dfNews.randomSplit([0.9, 0.1])\n",
                "df_train = splits[0]\n",
                "df_test = splits[1]\n",
                "\n"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.6-final",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}