{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "### Binary classification with Non-Deep learning algorithms: Naive Bayes and SVM\nWe will use Naive Bayes and Support Vector machine.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade numpy\n!pip install --upgrade pandas\n!pip install pyspark==2.4.5\n!pip install -U scikit-learn",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.19.2)\nRequirement already up-to-date: pandas in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.1.2)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: numpy>=1.15.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (1.19.2)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2018.9)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\nCollecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 171kB/s  eta 0:00:01    |\u2588\u2588\u2588\u2588\u258c                           | 30.3MB 38.4MB/s eta 0:00:05     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                   | 83.4MB 30.8MB/s eta 0:00:05     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589          | 148.3MB 45.8MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 34.9MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\nRequirement already up-to-date: scikit-learn in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.23.2)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.19.2)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (0.16.0)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()\n",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom time import time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport string\nimport re\nfrom datetime import datetime\nfrom packaging import version\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3\n\nfrom sklearn.model_selection import train_test_split",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Get our data from IBM Cloud\n\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_news = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-32e8ee67-397c-4ff1-b69b-543172331f43',\n    'IBM_API_KEY_ID': 'Rx4FR4JSAueCnnIsoevsgYgOsuh8LCXtbkFpFpC0EmVU',\n    #'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'ENDPOINT':'https://s3-api.us-geo.objectstorage.softlayer.net',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'advanceddatasciencecapstone-donotdelete-pr-tqabpnbxebk8rm',\n    'FILE': 'dfTrueFalseNews.pkl'\n}\n\ndef download_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.download_file(Bucket=credentials['BUCKET'],Key=key,Filename=local_file_name)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print('File Downloaded')\n\ndef upload_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.upload_file(Filename=local_file_name, Bucket=credentials['BUCKET'],Key=key)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print(' File Uploaded')\n        \ndfNews = download_file_cos(credentials_news, \"dfTrueFalseNews.pkl\", \"dfTrueFalseNews.pkl\")\ndfTrueFalseNews_tokenized  = download_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl','dfTrueFalseNews_tokenized.pkl')",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\nFile Downloaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfNewsTemp = pd.read_pickle('dfTrueFalseNews.pkl')\n#dfNews['truthvalue'] = pd.Categorical(dfNews['truthvalue'])\nx = dfNewsTemp['text'].values\n\ny = dfNewsTemp['truthvalue'].values\n\nprint (dfNewsTemp.shape, dfNewsTemp.columns, '\\n', dfNewsTemp.dtypes, type(x), type(y))\n",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(1126, 3) Index(['text', 'source', 'truthvalue'], dtype='object') \n text          object\nsource        object\ntruthvalue    object\ndtype: object <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "l = dfNewsTemp.index.tolist()\nd = {}\nfor i,v in enumerate(l):\n    d[v]=i\n#d['buzzfeed_line_20']\nl[999:]",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "['buzzfeed_line_20',\n 'buzzfeed_line_21',\n 'buzzfeed_line_22',\n 'buzzfeed_line_23',\n 'buzzfeed_line_24',\n 'buzzfeed_line_25',\n 'buzzfeed_line_27',\n 'buzzfeed_line_28',\n 'buzzfeed_line_30',\n 'buzzfeed_line_31',\n 'buzzfeed_line_32',\n 'snopes_line_54',\n 'snopes_line_59',\n 'snopes_line_65',\n 'snopes_line_68',\n 'snopes_line_69',\n 'snopes_line_73',\n 'snopes_line_75',\n 'snopes_line_77',\n 'snopes_line_82',\n 'snopes_line_85',\n 'snopes_line_88',\n 'snopes_line_93',\n 'snopes_line_101',\n 'snopes_line_115',\n 'snopes_line_121',\n 'snopes_line_122',\n 'snopes_line_123',\n 'snopes_line_125',\n 'snopes_line_126',\n 'snopes_line_127',\n 'snopes_line_129',\n 'snopes_line_131',\n 'snopes_line_132',\n 'snopes_line_133',\n 'snopes_line_134',\n 'snopes_line_137',\n 'snopes_line_140',\n 'snopes_line_141',\n 'snopes_line_142',\n 'snopes_line_143',\n 'snopes_line_145',\n 'snopes_line_149',\n 'snopes_line_150',\n 'snopes_line_151',\n 'snopes_line_152',\n 'snopes_line_153',\n 'snopes_line_154',\n 'snopes_line_155',\n 'snopes_line_156',\n 'snopes_line_158',\n 'snopes_line_159',\n 'snopes_line_162',\n 'snopes_line_164',\n 'snopes_line_165',\n 'snopes_line_166',\n 'snopes_line_167',\n 'snopes_line_169',\n 'snopes_line_170',\n 'snopes_line_172',\n 'snopes_line_173',\n 'snopes_line_293',\n 'snopes_line_294',\n 'snopes_line_295',\n 'snopes_line_296',\n 'snopes_line_297',\n 'snopes_line_298',\n 'snopes_line_299',\n 'snopes_line_300',\n 'snopes_line_301',\n 'snopes_line_302',\n 'snopes_line_303',\n 'snopes_line_304',\n 'snopes_line_305',\n 'snopes_line_306',\n 'snopes_line_307',\n 'snopes_line_50',\n 'snopes_line_51',\n 'snopes_line_52',\n 'snopes_line_55',\n 'snopes_line_56',\n 'snopes_line_57',\n 'snopes_line_58',\n 'snopes_line_60',\n 'snopes_line_61',\n 'snopes_line_62',\n 'snopes_line_63',\n 'snopes_line_64',\n 'snopes_line_66',\n 'snopes_line_67',\n 'snopes_line_70',\n 'snopes_line_71',\n 'snopes_line_72',\n 'snopes_line_74',\n 'snopes_line_76',\n 'snopes_line_78',\n 'snopes_line_79',\n 'snopes_line_80',\n 'snopes_line_81',\n 'snopes_line_83',\n 'snopes_line_84',\n 'snopes_line_86',\n 'snopes_line_87',\n 'snopes_line_90',\n 'snopes_line_92',\n 'snopes_line_94',\n 'snopes_line_95',\n 'snopes_line_96',\n 'snopes_line_97',\n 'snopes_line_98',\n 'snopes_line_100',\n 'snopes_line_102',\n 'snopes_line_103',\n 'snopes_line_104',\n 'snopes_line_106',\n 'snopes_line_107',\n 'snopes_line_108',\n 'snopes_line_109',\n 'snopes_line_110',\n 'snopes_line_111',\n 'snopes_line_112',\n 'snopes_line_113',\n 'snopes_line_114',\n 'snopes_line_116',\n 'snopes_line_117',\n 'snopes_line_118',\n 'snopes_line_311']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Prepare the Text\n1. Change all the text to lower case\n2. Word Tokenization\n3. Remove Stop words\n4. Remove Non-alpha text\n5. Word Lemmatization"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.corpus import wordnet as wn\nfrom collections import defaultdict\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\n\n# reproduce the same result every time the script is run.\nnp.random.seed(500)",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/dsxuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Step 1: Change all the text to lower case.\ndfNewsTemp['text'] = [entry.lower() for entry in dfNewsTemp['text']]",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Step 2: Tokenization : Each entry will be broken into set of words\ndfNewsTemp['text']= [word_tokenize(entry) for entry in dfNewsTemp['text']]",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Step 3,4,5: Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Initializing WordNetLemmatizer()\nword_Lemmatized = WordNetLemmatizer()\n#dfNewsTemp.loc['text_final'].astype('object')\ncol = []\nfor index,entry in enumerate(dfNewsTemp['text']):\n    # Declaring Empty List to store the words that follow the rules for this step\n    Final_words = []\n    # Initializing WordNetLemmatizer()\n    #word_Lemmatized = WordNetLemmatizer()\n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n    for word, tag in pos_tag(entry):\n        # Below condition is to check for Stop words and consider only alphabets\n        #if word not in stopwords.words('english') and word.isalpha():\n        if word.isalpha():\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_Final)\n    # The final processed set of words for each iteration will be stored in 'text_final'\n    \n    #dfNewsTemp.loc[index,'text_final'] = \" \".join(Final_words)\n    #dfNewsTemp.loc[index,'text_final'] = Final_words\n    col.append(\" \".join(Final_words))\ndfNewsTemp['text_final'] = col\n",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#dfNewsTemp['text_final'] = col\n#col[-1]",
            "execution_count": 23,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# This took a while, so let's save the result.\ndfNewsTemp.to_pickle('dfTrueFalseNews_tokenized.pkl', protocol=4 )\n\n# save to our cloud storage \nupload_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl','dfTrueFalseNews_tokenized.pkl')",
            "execution_count": 36,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": " File Uploaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfTrueFalseNews_tokenized  = download_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl', 'dfTrueFalseNews_tokenized.pkl')\n!ls -al",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\ntotal 33968\ndrwxr-x--- 3 dsxuser dsxuser     4096 Sep 24 01:06 .\ndrwx------ 1 dsxuser dsxuser     4096 Sep 24 00:48 ..\n-rw-r----- 1 dsxuser dsxuser  2228457 Sep 24 01:06 dfTrueFalseNews.pkl\n-rw-r----- 1 dsxuser dsxuser  5159678 Sep 24 01:06 dfTrueFalseNews_tokenized.pkl\n-rw-r----- 1 dsxuser dsxuser 27357210 Sep 24 00:50 embedding_word2vec.model\n-rw-r----- 1 dsxuser dsxuser     9204 Sep 24 00:51 model_evaluation_utils.py\ndrwxr-x--- 2 dsxuser dsxuser     4096 Sep 24 00:51 __pycache__\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfTrueFalseNews_tokenized = pd.read_pickle('dfTrueFalseNews_tokenized.pkl')\ndfTrueFalseNews_tokenized.head()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "                                                           text  \\\ntech003legit  [a, google, computer, victorious, over, the, w...   \npolit11legit  [white, house, keeps, up, sanctuary, cities, p...   \nbiz40legit    [why, silicon, valley, is, n't, fighting, to, ...   \nedu10legit    [protesters, disrupt, devos, school, visit, pr...   \ntech038legit  [solar-powered, 'skin, ', could, make, prosthe...   \n\n                         source truthvalue  \\\ntech003legit  MihalceaNewsLegit          1   \npolit11legit  MihalceaNewsLegit          1   \nbiz40legit    MihalceaNewsLegit          1   \nedu10legit    MihalceaNewsLegit          1   \ntech038legit  MihalceaNewsLegit          1   \n\n                                                     text_final  \ntech003legit  a google computer victorious over the world ch...  \npolit11legit  white house keep up sanctuary city pressure wi...  \nbiz40legit    why silicon valley be fight to save the intern...  \nedu10legit    protester disrupt devos school visit protester...  \ntech038legit  could make prosthetics more real many people t...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>[a, google, computer, victorious, over, the, w...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>a google computer victorious over the world ch...</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>[white, house, keeps, up, sanctuary, cities, p...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>white house keep up sanctuary city pressure wi...</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>[why, silicon, valley, is, n't, fighting, to, ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>why silicon valley be fight to save the intern...</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>[protesters, disrupt, devos, school, visit, pr...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>protester disrupt devos school visit protester...</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>[solar-powered, 'skin, ', could, make, prosthe...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>could make prosthetics more real many people t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from sklearn import model_selection\nTrain_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dfTrueFalseNews_tokenized['text_final'],dfTrueFalseNews_tokenized['truthvalue'],test_size=0.1, \n                                                                    stratify=dfTrueFalseNews_tokenized['truthvalue'], random_state=42)\n#print(dfTrueFalseNews_tokenized.shape, (dfTrueFalseNews_tokenized['text_final'].shape), (dfTrueFalseNews_tokenized['truthvalue'].shape))\n#dfTrueFalseNews_tokenized['text_final'][0]",
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#corpus = [\" \".join(i) for i in dfTrueFalseNews_tokenized['text_final']]\n#corpus[1:3]\ndfTrueFalseNews_tokenized.head()\n\n#dfTrueFalseNews_tokenized.dtypes\n#import sklearn\n#sklearn.show_versions()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "                                                           text  \\\ntech003legit  [a, google, computer, victorious, over, the, w...   \npolit11legit  [white, house, keeps, up, sanctuary, cities, p...   \nbiz40legit    [why, silicon, valley, is, n't, fighting, to, ...   \nedu10legit    [protesters, disrupt, devos, school, visit, pr...   \ntech038legit  [solar-powered, 'skin, ', could, make, prosthe...   \n\n                         source truthvalue  \\\ntech003legit  MihalceaNewsLegit          1   \npolit11legit  MihalceaNewsLegit          1   \nbiz40legit    MihalceaNewsLegit          1   \nedu10legit    MihalceaNewsLegit          1   \ntech038legit  MihalceaNewsLegit          1   \n\n                                                     text_final  \ntech003legit  a google computer victorious over the world ch...  \npolit11legit  white house keep up sanctuary city pressure wi...  \nbiz40legit    why silicon valley be fight to save the intern...  \nedu10legit    protester disrupt devos school visit protester...  \ntech038legit  could make prosthetics more real many people t...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>[a, google, computer, victorious, over, the, w...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>a google computer victorious over the world ch...</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>[white, house, keeps, up, sanctuary, cities, p...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>white house keep up sanctuary city pressure wi...</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>[why, silicon, valley, is, n't, fighting, to, ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>why silicon valley be fight to save the intern...</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>[protesters, disrupt, devos, school, visit, pr...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>protester disrupt devos school visit protester...</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>[solar-powered, 'skin, ', could, make, prosthe...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>could make prosthetics more real many people t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nTfidf_vect = TfidfVectorizer(max_features=26000)\n\nTfidf_vect.fit(dfTrueFalseNews_tokenized['text_final'])\n\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "print(len(Tfidf_vect.vocabulary_), Test_X_Tfidf)\n",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "16836   (0, 16781)\t0.17118384142343043\n  (0, 16639)\t0.04185565787113178\n  (0, 16600)\t0.05954997292225774\n  (0, 16589)\t0.02301154925052861\n  (0, 16567)\t0.08131469889955778\n  (0, 16538)\t0.12265734559103758\n  (0, 16529)\t0.03399451527586988\n  (0, 16348)\t0.04530593722490369\n  (0, 16327)\t0.12265734559103758\n  (0, 16326)\t0.24531469118207516\n  (0, 16310)\t0.08738577707399033\n  (0, 16259)\t0.12265734559103758\n  (0, 15919)\t0.08998106727609378\n  (0, 15783)\t0.08311653634401397\n  (0, 15485)\t0.0504735652596911\n  (0, 15455)\t0.11038670810281244\n  (0, 15357)\t0.09965641456221547\n  (0, 15216)\t0.09066031866794538\n  (0, 15209)\t0.08075265842869195\n  (0, 15099)\t0.058575261367563684\n  (0, 15059)\t0.03146386007079189\n  (0, 15015)\t0.1603177735792545\n  (0, 15012)\t0.021586417845775294\n  (0, 15004)\t0.04339600181875995\n  (0, 14908)\t0.03564632225517207\n  :\t:\n  (112, 2558)\t0.06389203383775867\n  (112, 2260)\t0.04005697123346543\n  (112, 2259)\t0.04162023946962601\n  (112, 2116)\t0.018574914916278124\n  (112, 2097)\t0.01801133237170175\n  (112, 1920)\t0.035192909639914935\n  (112, 1787)\t0.05604061535041035\n  (112, 1703)\t0.044978003651548734\n  (112, 1483)\t0.0327419105607719\n  (112, 1361)\t0.027776768097264173\n  (112, 1356)\t0.04418211000965616\n  (112, 1332)\t0.22896854328619334\n  (112, 950)\t0.016781207279768385\n  (112, 716)\t0.05355529337226519\n  (112, 708)\t0.04060038495352788\n  (112, 627)\t0.2951329081125453\n  (112, 582)\t0.09256836860171745\n  (112, 565)\t0.03672988426405744\n  (112, 488)\t0.02278677092687436\n  (112, 478)\t0.04804547946502907\n  (112, 439)\t0.021947319263442573\n  (112, 296)\t0.020949984755073037\n  (112, 224)\t0.06173007463466115\n  (112, 155)\t0.03856972903933744\n  (112, 45)\t0.020346541204296853\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Use Naive Bayes Classifier"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# some open source utilities to evaluate a model.\n# you can see them at: https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch05_Building_Tuning_and_Deploying_Models/model_evaluation_utils.py\ndownload_file_cos(credentials_news,'model_evaluation_utils.py', 'model_evaluation_utils.py')\nimport importlib\n#importlib.reload(model_evaluation_utils)\nimport model_evaluation_utils as meu",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "File Downloaded\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import naive_bayes\nfrom sklearn.metrics import accuracy_score\n# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nTrain_y_int = Train_Y.astype('int')\nTest_y_int = Test_Y.astype('int')\nNaive.fit(Train_X_Tfidf,Train_y_int)# predict the labels on validation dataset\n#Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\npredictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_y_int)* 100)\nprint(\"Naive model params: {}\".format(Naive.get_params()))\nmeu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_NB.tolist())",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Naive Bayes Accuracy Score ->  51.32743362831859\nNaive model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.5133\nPrecision: 0.513\nRecall: 0.5133\nF1 Score: 0.5062\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.51      0.39      0.44        56\n           0       0.51      0.63      0.57        57\n\n    accuracy                           0.51       113\n   macro avg       0.51      0.51      0.51       113\nweighted avg       0.51      0.51      0.51       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1         22  34\n        0         21  36\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Discussion\nThis is a Naive Bayes model using a Bag of Words TF-IDF encoding. \n\nThe result of 51% accuracy is not very good.  "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Use Support Vector Machine"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import svm\n# Classifier - Algorithm - SVM\n# fit the training dataset on the classifier\nSVM = svm.SVC(C=2.5, kernel='sigmoid', degree=3, gamma='scale')\nSVM.fit(Train_X_Tfidf,Train_y_int)# predict the labels on validation dataset\npredictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\nprint(\"SVM Accuracy Score -> {}\".format(accuracy_score(predictions_SVM, Test_y_int)*100))\nprint(\"SVM model params: {}\".format(Naive.get_params()))\nmeu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_SVM.tolist())",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "SVM Accuracy Score -> 59.29203539823009\nSVM model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.5929\nPrecision: 0.6035\nRecall: 0.5929\nF1 Score: 0.5889\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.64      0.49      0.56        59\n           0       0.56      0.70      0.62        54\n\n    accuracy                           0.59       113\n   macro avg       0.60      0.60      0.59       113\nweighted avg       0.60      0.59      0.59       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1         29  30\n        0         16  38\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Discussion\n\nThis is a Support Vector Machine using the same Bag of Words TF-IDF encoding as above and does surprisingly better, with a 60% accuracy rate that rivals others results.\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Split the data"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# use the dataframe to write a parquet file\ndfNewsTemp.to_parquet('dfTrueFalseNews_tokenized.parquet')\ndfNews = spark.read.parquet('dfTrueFalseNews_tokenized.parquet')\ndfNews.createOrReplaceTempView('dfNews')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dfNewsTemp['text_final'],Corpus['label'],test_size=0.1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = dfNews.randomSplit([0.9, 0.1])\ndf_train = splits[0]\ndf_test = splits[1]\n\n",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}