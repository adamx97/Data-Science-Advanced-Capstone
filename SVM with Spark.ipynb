{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Binary classification with a Non-Deep learning algorithm \nWe will use Support Vector machine with Spark\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting numpy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5MB 9.9MB/s eta 0:00:01\n\u001b[31mERROR: tensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mERROR: autoai-libs 1.10.5 has requirement pandas>=0.24.2, but you'll have pandas 0.24.1 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Found existing installation: numpy 1.15.4\n    Uninstalling numpy-1.15.4:\n      Successfully uninstalled numpy-1.15.4\nSuccessfully installed numpy-1.19.2\nCollecting pandas\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/11/e1f53db0614f2721027aab297c8afd2eaf58d33d566441a97ea454541c5e/pandas-1.1.2-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.5MB 8.8MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (1.19.2)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2018.9)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n\u001b[31mERROR: ibm-watson-machine-learning 1.0.10 has requirement pandas<=0.25.3, but you'll have pandas 1.1.2 which is incompatible.\u001b[0m\nInstalling collected packages: pandas\n  Found existing installation: pandas 0.24.1\n    Uninstalling pandas-0.24.1:\n      Successfully uninstalled pandas-0.24.1\nSuccessfully installed pandas-1.1.2\nCollecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 9.5MB/s eta 0:00:011     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f             | 123.3MB 42.5MB/s eta 0:00:03\ufffd\ufffd\u2588\u2588\u2589           | 141.8MB 42.5MB/s eta 0:00:02     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          | 149.1MB 51.3MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 50.6MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\nCollecting scikit-learn\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 10.1MB/s eta 0:00:01\n\u001b[?25hCollecting joblib>=0.11 (from scikit-learn)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/dd/0e015051b4a27ec5a58b02ab774059f3289a94b0906f880a3f9507e74f38/joblib-0.16.0-py3-none-any.whl (300kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 38.7MB/s eta 0:00:01\n\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn)\n  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.19.2)\n\u001b[31mERROR: autoai-libs 1.10.5 has requirement scikit-learn==0.20.3, but you'll have scikit-learn 0.23.2 which is incompatible.\u001b[0m\nInstalling collected packages: joblib, threadpoolctl, scikit-learn\n  Found existing installation: scikit-learn 0.20.3\n    Uninstalling scikit-learn-0.20.3:\n      Successfully uninstalled scikit-learn-0.20.3\nSuccessfully installed joblib-0.16.0 scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
                }
            ],
            "source": "!pip install --upgrade numpy\n!pip install --upgrade pandas\n!pip install pyspark==2.4.5\n!pip install -U scikit-learn"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom time import time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport string\nimport re\nfrom datetime import datetime\nfrom packaging import version\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3\n\nfrom sklearn.model_selection import train_test_split"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "File Downloaded\nFile Downloaded\n"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(1126, 3) Index(['text', 'source', 'truthvalue'], dtype='object') \n text          object\nsource        object\ntruthvalue    object\ndtype: object <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
                }
            ],
            "source": "dfNewsTemp = pd.read_pickle('dfTrueFalseNews.pkl')\n#dfNews['truthvalue'] = pd.Categorical(dfNews['truthvalue'])\nx = dfNewsTemp['text'].values\n\ny = dfNewsTemp['truthvalue'].values\n\nprint (dfNewsTemp.shape, dfNewsTemp.columns, '\\n', dfNewsTemp.dtypes, type(x), type(y))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['buzzfeed_line_20',\n 'buzzfeed_line_21',\n 'buzzfeed_line_22',\n 'buzzfeed_line_23',\n 'buzzfeed_line_24',\n 'buzzfeed_line_25',\n 'buzzfeed_line_27',\n 'buzzfeed_line_28',\n 'buzzfeed_line_30',\n 'buzzfeed_line_31',\n 'buzzfeed_line_32',\n 'snopes_line_54',\n 'snopes_line_59',\n 'snopes_line_65',\n 'snopes_line_68',\n 'snopes_line_69',\n 'snopes_line_73',\n 'snopes_line_75',\n 'snopes_line_77',\n 'snopes_line_82',\n 'snopes_line_85',\n 'snopes_line_88',\n 'snopes_line_93',\n 'snopes_line_101',\n 'snopes_line_115',\n 'snopes_line_121',\n 'snopes_line_122',\n 'snopes_line_123',\n 'snopes_line_125',\n 'snopes_line_126',\n 'snopes_line_127',\n 'snopes_line_129',\n 'snopes_line_131',\n 'snopes_line_132',\n 'snopes_line_133',\n 'snopes_line_134',\n 'snopes_line_137',\n 'snopes_line_140',\n 'snopes_line_141',\n 'snopes_line_142',\n 'snopes_line_143',\n 'snopes_line_145',\n 'snopes_line_149',\n 'snopes_line_150',\n 'snopes_line_151',\n 'snopes_line_152',\n 'snopes_line_153',\n 'snopes_line_154',\n 'snopes_line_155',\n 'snopes_line_156',\n 'snopes_line_158',\n 'snopes_line_159',\n 'snopes_line_162',\n 'snopes_line_164',\n 'snopes_line_165',\n 'snopes_line_166',\n 'snopes_line_167',\n 'snopes_line_169',\n 'snopes_line_170',\n 'snopes_line_172',\n 'snopes_line_173',\n 'snopes_line_293',\n 'snopes_line_294',\n 'snopes_line_295',\n 'snopes_line_296',\n 'snopes_line_297',\n 'snopes_line_298',\n 'snopes_line_299',\n 'snopes_line_300',\n 'snopes_line_301',\n 'snopes_line_302',\n 'snopes_line_303',\n 'snopes_line_304',\n 'snopes_line_305',\n 'snopes_line_306',\n 'snopes_line_307',\n 'snopes_line_50',\n 'snopes_line_51',\n 'snopes_line_52',\n 'snopes_line_55',\n 'snopes_line_56',\n 'snopes_line_57',\n 'snopes_line_58',\n 'snopes_line_60',\n 'snopes_line_61',\n 'snopes_line_62',\n 'snopes_line_63',\n 'snopes_line_64',\n 'snopes_line_66',\n 'snopes_line_67',\n 'snopes_line_70',\n 'snopes_line_71',\n 'snopes_line_72',\n 'snopes_line_74',\n 'snopes_line_76',\n 'snopes_line_78',\n 'snopes_line_79',\n 'snopes_line_80',\n 'snopes_line_81',\n 'snopes_line_83',\n 'snopes_line_84',\n 'snopes_line_86',\n 'snopes_line_87',\n 'snopes_line_90',\n 'snopes_line_92',\n 'snopes_line_94',\n 'snopes_line_95',\n 'snopes_line_96',\n 'snopes_line_97',\n 'snopes_line_98',\n 'snopes_line_100',\n 'snopes_line_102',\n 'snopes_line_103',\n 'snopes_line_104',\n 'snopes_line_106',\n 'snopes_line_107',\n 'snopes_line_108',\n 'snopes_line_109',\n 'snopes_line_110',\n 'snopes_line_111',\n 'snopes_line_112',\n 'snopes_line_113',\n 'snopes_line_114',\n 'snopes_line_116',\n 'snopes_line_117',\n 'snopes_line_118',\n 'snopes_line_311']"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "l = dfNewsTemp.index.tolist()\nd = {}\nfor i,v in enumerate(l):\n    d[v]=i\n#d['buzzfeed_line_20']\nl[999:]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Prepare the Text\n1. Change all the text to lower case\n2. Word Tokenization\n3. Remove Stop words\n4. Remove Non-alpha text\n5. Word Lemmatization"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
                }
            ],
            "source": "from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.corpus import wordnet as wn\nfrom collections import defaultdict\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\n\n# reproduce the same result every time the script is run.\nnp.random.seed(500)"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "str"
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#dfNewsTemp.loc['snopes_line_311','text_final'] = \" \".join(['face', 'dog', 'cat'])\n#dfNewsTemp.astype({'text_final':'object'})\n#dfNewsTemp.tail()\n#dfNewsTemp.dtypes\n#dfNewsTemp.loc[0,'text_value']=list([1,2,3])\n#dfNewsTemp['text']\n#ctr=0\n# for index,entry in enumerate(dfNewsTemp['text']):\n#     if ctr < 3: \n#         print (index, \" \", entry)\n#     ctr +=1\n#dfNewsTemp['text_final'][0]\ntype(dfNewsTemp.loc['snopes_line_311','text_final'])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# Step 1: Change all the text to lower case.\ndfNewsTemp['text'] = [entry.lower() for entry in dfNewsTemp['text']]"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "# Step 2: Tokenization : Each entry will be broken into set of words\ndfNewsTemp['text']= [word_tokenize(entry) for entry in dfNewsTemp['text']]"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "# Step 3,4,5: Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "# Initializing WordNetLemmatizer()\nword_Lemmatized = WordNetLemmatizer()\n#dfNewsTemp.loc['text_final'].astype('object')\ncol = []\nfor index,entry in enumerate(dfNewsTemp['text']):\n    # Declaring Empty List to store the words that follow the rules for this step\n    Final_words = []\n    # Initializing WordNetLemmatizer()\n    #word_Lemmatized = WordNetLemmatizer()\n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n    for word, tag in pos_tag(entry):\n        # Below condition is to check for Stop words and consider only alphabets\n        #if word not in stopwords.words('english') and word.isalpha():\n        if word.isalpha():\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_Final)\n    # The final processed set of words for each iteration will be stored in 'text_final'\n    \n    #dfNewsTemp.loc[index,'text_final'] = \" \".join(Final_words)\n    #dfNewsTemp.loc[index,'text_final'] = Final_words\n    col.append(\" \".join(Final_words))\ndfNewsTemp['text_final'] = col\n"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "#dfNewsTemp['text_final'] = col\n#col[-1]"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": " File Uploaded\n"
                }
            ],
            "source": "# This took a while, so let's save the result.\ndfNewsTemp.to_pickle('dfTrueFalseNews_tokenized.pkl', protocol=4 )\n\n# save to our cloud storage \nupload_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl','dfTrueFalseNews_tokenized.pkl')"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "File Downloaded\ntotal 7228\ndrwxr-x--- 2 dsxuser dsxuser    4096 Sep 14 21:41 .\ndrwx------ 1 dsxuser dsxuser    4096 Sep 14 21:41 ..\n-rw-r----- 1 dsxuser dsxuser 2228457 Sep 14 21:41 dfTrueFalseNews.pkl\n-rw-r----- 1 dsxuser dsxuser 5159678 Sep 14 21:41 dfTrueFalseNews_tokenized.pkl\n"
                }
            ],
            "source": "dfTrueFalseNews_tokenized  = download_file_cos(credentials_news,'dfTrueFalseNews_tokenized.pkl', 'dfTrueFalseNews_tokenized.pkl')\n!ls -al"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>[a, google, computer, victorious, over, the, w...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>a google computer victorious over the world ch...</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>[white, house, keeps, up, sanctuary, cities, p...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>white house keep up sanctuary city pressure wi...</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>[why, silicon, valley, is, n't, fighting, to, ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>why silicon valley be fight to save the intern...</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>[protesters, disrupt, devos, school, visit, pr...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>protester disrupt devos school visit protester...</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>[solar-powered, 'skin, ', could, make, prosthe...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>could make prosthetics more real many people t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                                           text  \\\ntech003legit  [a, google, computer, victorious, over, the, w...   \npolit11legit  [white, house, keeps, up, sanctuary, cities, p...   \nbiz40legit    [why, silicon, valley, is, n't, fighting, to, ...   \nedu10legit    [protesters, disrupt, devos, school, visit, pr...   \ntech038legit  [solar-powered, 'skin, ', could, make, prosthe...   \n\n                         source truthvalue  \\\ntech003legit  MihalceaNewsLegit          1   \npolit11legit  MihalceaNewsLegit          1   \nbiz40legit    MihalceaNewsLegit          1   \nedu10legit    MihalceaNewsLegit          1   \ntech038legit  MihalceaNewsLegit          1   \n\n                                                     text_final  \ntech003legit  a google computer victorious over the world ch...  \npolit11legit  white house keep up sanctuary city pressure wi...  \nbiz40legit    why silicon valley be fight to save the intern...  \nedu10legit    protester disrupt devos school visit protester...  \ntech038legit  could make prosthetics more real many people t...  "
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "dfTrueFalseNews_tokenized = pd.read_pickle('dfTrueFalseNews_tokenized.pkl')\ndfTrueFalseNews_tokenized.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "from sklearn import model_selection\nTrain_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dfTrueFalseNews_tokenized['text_final'],dfTrueFalseNews_tokenized['truthvalue'],test_size=0.1)\n#print(dfTrueFalseNews_tokenized.shape, (dfTrueFalseNews_tokenized['text_final'].shape), (dfTrueFalseNews_tokenized['truthvalue'].shape))\n#dfTrueFalseNews_tokenized['text_final'][0]"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n      <th>truthvalue</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tech003legit</th>\n      <td>[a, google, computer, victorious, over, the, w...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>a google computer victorious over the world ch...</td>\n    </tr>\n    <tr>\n      <th>polit11legit</th>\n      <td>[white, house, keeps, up, sanctuary, cities, p...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>white house keep up sanctuary city pressure wi...</td>\n    </tr>\n    <tr>\n      <th>biz40legit</th>\n      <td>[why, silicon, valley, is, n't, fighting, to, ...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>why silicon valley be fight to save the intern...</td>\n    </tr>\n    <tr>\n      <th>edu10legit</th>\n      <td>[protesters, disrupt, devos, school, visit, pr...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>protester disrupt devos school visit protester...</td>\n    </tr>\n    <tr>\n      <th>tech038legit</th>\n      <td>[solar-powered, 'skin, ', could, make, prosthe...</td>\n      <td>MihalceaNewsLegit</td>\n      <td>1</td>\n      <td>could make prosthetics more real many people t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                                           text  \\\ntech003legit  [a, google, computer, victorious, over, the, w...   \npolit11legit  [white, house, keeps, up, sanctuary, cities, p...   \nbiz40legit    [why, silicon, valley, is, n't, fighting, to, ...   \nedu10legit    [protesters, disrupt, devos, school, visit, pr...   \ntech038legit  [solar-powered, 'skin, ', could, make, prosthe...   \n\n                         source truthvalue  \\\ntech003legit  MihalceaNewsLegit          1   \npolit11legit  MihalceaNewsLegit          1   \nbiz40legit    MihalceaNewsLegit          1   \nedu10legit    MihalceaNewsLegit          1   \ntech038legit  MihalceaNewsLegit          1   \n\n                                                     text_final  \ntech003legit  a google computer victorious over the world ch...  \npolit11legit  white house keep up sanctuary city pressure wi...  \nbiz40legit    why silicon valley be fight to save the intern...  \nedu10legit    protester disrupt devos school visit protester...  \ntech038legit  could make prosthetics more real many people t...  "
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#corpus = [\" \".join(i) for i in dfTrueFalseNews_tokenized['text_final']]\n#corpus[1:3]\ndfTrueFalseNews_tokenized.head()\n\n#dfTrueFalseNews_tokenized.dtypes\n#import sklearn\n#sklearn.show_versions()"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nTfidf_vect = TfidfVectorizer(max_features=26000)\n\nTfidf_vect.fit(dfTrueFalseNews_tokenized['text_final'])\n\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "16836   (0, 16781)\t0.17118384142343043\n  (0, 16639)\t0.04185565787113178\n  (0, 16600)\t0.05954997292225774\n  (0, 16589)\t0.02301154925052861\n  (0, 16567)\t0.08131469889955778\n  (0, 16538)\t0.12265734559103758\n  (0, 16529)\t0.03399451527586988\n  (0, 16348)\t0.04530593722490369\n  (0, 16327)\t0.12265734559103758\n  (0, 16326)\t0.24531469118207516\n  (0, 16310)\t0.08738577707399033\n  (0, 16259)\t0.12265734559103758\n  (0, 15919)\t0.08998106727609378\n  (0, 15783)\t0.08311653634401397\n  (0, 15485)\t0.0504735652596911\n  (0, 15455)\t0.11038670810281244\n  (0, 15357)\t0.09965641456221547\n  (0, 15216)\t0.09066031866794538\n  (0, 15209)\t0.08075265842869195\n  (0, 15099)\t0.058575261367563684\n  (0, 15059)\t0.03146386007079189\n  (0, 15015)\t0.1603177735792545\n  (0, 15012)\t0.021586417845775294\n  (0, 15004)\t0.04339600181875995\n  (0, 14908)\t0.03564632225517207\n  :\t:\n  (112, 2558)\t0.06389203383775867\n  (112, 2260)\t0.04005697123346543\n  (112, 2259)\t0.04162023946962601\n  (112, 2116)\t0.018574914916278124\n  (112, 2097)\t0.01801133237170175\n  (112, 1920)\t0.035192909639914935\n  (112, 1787)\t0.05604061535041035\n  (112, 1703)\t0.044978003651548734\n  (112, 1483)\t0.0327419105607719\n  (112, 1361)\t0.027776768097264173\n  (112, 1356)\t0.04418211000965616\n  (112, 1332)\t0.22896854328619334\n  (112, 950)\t0.016781207279768385\n  (112, 716)\t0.05355529337226519\n  (112, 708)\t0.04060038495352788\n  (112, 627)\t0.2951329081125453\n  (112, 582)\t0.09256836860171745\n  (112, 565)\t0.03672988426405744\n  (112, 488)\t0.02278677092687436\n  (112, 478)\t0.04804547946502907\n  (112, 439)\t0.021947319263442573\n  (112, 296)\t0.020949984755073037\n  (112, 224)\t0.06173007463466115\n  (112, 155)\t0.03856972903933744\n  (112, 45)\t0.020346541204296853\n"
                }
            ],
            "source": "print(len(Tfidf_vect.vocabulary_), Test_X_Tfidf)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Use Naive Bayes Classifier"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "File Downloaded\n"
                }
            ],
            "source": "# some open source utilities to evaluate a model.\n# you can see them at: https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch05_Building_Tuning_and_Deploying_Models/model_evaluation_utils.py\ndownload_file_cos(credentials_news,'model_evaluation_utils.py', 'model_evaluation_utils.py')\nimport importlib\n#importlib.reload(model_evaluation_utils)\nimport model_evaluation_utils as meu"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Naive Bayes Accuracy Score ->  36.283185840707965\nNaive model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.3628\nPrecision: 0.3381\nRecall: 0.3628\nF1 Score: 0.3293\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.29      0.15      0.20        59\n           0       0.39      0.59      0.47        54\n\n    accuracy                           0.36       113\n   macro avg       0.34      0.37      0.34       113\nweighted avg       0.34      0.36      0.33       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1          9  50\n        0         22  32\n"
                }
            ],
            "source": "from sklearn import naive_bayes\nfrom sklearn.metrics import accuracy_score\n# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nTrain_y_int = Train_Y.astype('int')\nTest_y_int = Test_Y.astype('int')\nNaive.fit(Train_X_Tfidf,Train_y_int)# predict the labels on validation dataset\n#Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\npredictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_y_int)* 100)\nprint(\"Naive model params: {}\".format(Naive.get_params()))\nmeu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_NB.tolist())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Use Support Vector Machine"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "SVM Accuracy Score -> 59.29203539823009\nSVM model params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\nModel Performance metrics:\n------------------------------\nAccuracy: 0.5929\nPrecision: 0.6035\nRecall: 0.5929\nF1 Score: 0.5889\n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n           1       0.64      0.49      0.56        59\n           0       0.56      0.70      0.62        54\n\n    accuracy                           0.59       113\n   macro avg       0.60      0.60      0.59       113\nweighted avg       0.60      0.59      0.59       113\n\n\nPrediction Confusion Matrix:\n------------------------------\n          Predicted:    \n                   1   0\nActual: 1         29  30\n        0         16  38\n"
                }
            ],
            "source": "from sklearn import svm\n# Classifier - Algorithm - SVM\n# fit the training dataset on the classifier\nSVM = svm.SVC(C=2.5, kernel='sigmoid', degree=3, gamma='scale')\nSVM.fit(Train_X_Tfidf,Train_y_int)# predict the labels on validation dataset\npredictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\nprint(\"SVM Accuracy Score -> {}\".format(accuracy_score(predictions_SVM, Test_y_int)*100))\nprint(\"SVM model params: {}\".format(Naive.get_params()))\nmeu.display_model_performance_metrics(true_labels=Test_Y.tolist(), predicted_labels=predictions_SVM.tolist())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Split the data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# use the dataframe to write a parquet file\ndfNewsTemp.to_parquet('dfTrueFalseNews_tokenized.parquet')\ndfNews = spark.read.parquet('dfTrueFalseNews_tokenized.parquet')\ndfNews.createOrReplaceTempView('dfNews')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dfNewsTemp['text_final'],Corpus['label'],test_size=0.1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### Prepare the data for Spark"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "splits = dfNews.randomSplit([0.9, 0.1])\ndf_train = splits[0]\ndf_test = splits[1]\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfNews.show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\n\nindexer = StringIndexer(inputCol=\"text\", outputCol=\"text_index\")\nencoder = OneHotEncoder(inputCol=\"text_index\", outputCol=\"textVec\")\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}